{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aflw4PblpFZX"
      },
      "source": [
        "# Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnI8e0yg6e03",
        "outputId": "fea6cc3d-1a10-4540-cc9c-96889d93bbc4"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPeCIGQn6IgK",
        "outputId": "856a630c-aacf-4111-b7c8-508e961c903d"
      },
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUzdMrv2_4Ic",
        "outputId": "7e4f02a8-9a25-44b9-cef0-e79115220e8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p76vsXXwLL9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf1l405hwN6k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "e5833a4f-5d2b-4ed8-9529-2ab76c3b0bc8"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/test.csv')\n",
        "test #프로토타입으로 프롬프트 실험 진행용 50개 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO0MpuiwwsbQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# ====== 숨은 문자 제거 ======\n",
        "def norm_line(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    return (s.replace(\"\\xa0\", \" \")\n",
        "              .replace(\"\\u200b\", \"\")\n",
        "              .replace(\"\\u200c\", \"\")\n",
        "              .replace(\"\\u200d\", \"\")\n",
        "              .replace(\"\\ufeff\", \"\"))\n",
        "\n",
        "# ====== 객관식 선지 패턴 ======\n",
        "OPTION_LINE_RE = re.compile(\n",
        "    r'^[\\s\\u200b\\u200c\\u200d\\ufeff]*'              # 선행 공백/제로폭\n",
        "    r'(?:'\n",
        "    r'  \\(\\d{1,2}\\)'                               # (1) ~ (12)\n",
        "    r' | \\d{1}\\s+'                                 # 1␠ 내용 (1자리+공백 허용)\n",
        "    r' | \\d{1,2}[.)]\\s*'                           # 1. / 1) / 12. / 12)\n",
        "    r' | [①-⑳]\\s*'                                 # ①~⑳\n",
        "    r' | [A-F][.)]\\s*'                              # A. / A) (대문자만, 공백 불허)\n",
        "    r')',\n",
        "    re.X\n",
        ")\n",
        "\n",
        "def count_options(question_text: str) -> int:\n",
        "    \"\"\"\n",
        "    줄 단위로 선지 패턴에 매칭되는 라인 개수\n",
        "    \"\"\"\n",
        "    cnt = 0\n",
        "    for ln in str(question_text).splitlines():\n",
        "        ln = norm_line(ln)\n",
        "        if OPTION_LINE_RE.match(ln):\n",
        "            cnt += 1\n",
        "    return cnt\n",
        "\n",
        "def is_multiple_choice(question_text: str) -> bool:\n",
        "    \"\"\"\n",
        "    객관식 여부 판단 (기본: 선지 패턴 2개 이상)\n",
        "    \"\"\"\n",
        "    cnt = count_options(question_text)\n",
        "    return cnt >= 2\n",
        "\n",
        "def strip_option_prefix(line: str) -> str:\n",
        "    \"\"\"\n",
        "    '1) 내용', '(2) 내용', 'A. 내용', '① 내용'에서 접두 기호 제거\n",
        "    \"\"\"\n",
        "    m = OPTION_LINE_RE.match(line)\n",
        "    if not m:\n",
        "        return line.strip()\n",
        "    return line[m.end():].strip()\n",
        "\n",
        "def extract_question_and_choices(full_text: str):\n",
        "    lines = [norm_line(ln).rstrip() for ln in str(full_text).strip().split(\"\\n\")]\n",
        "    q_lines, options = [], []\n",
        "    for line in lines:\n",
        "        if OPTION_LINE_RE.match(line):\n",
        "            options.append(strip_option_prefix(line))\n",
        "        else:\n",
        "            q_lines.append(line.strip())\n",
        "    question = re.sub(r\"\\s+\", \" \", \" \".join([ln for ln in q_lines if ln]))\n",
        "    return question, options\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mV0X8A85y8B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sentence_transformers import util\n",
        "def _trim_to_sentence(text, max_chars):\n",
        "    \"\"\"\n",
        "    최대 max_chars 길이까지 잘라내되 문장 경계(마침표, 개행 등) 기준으로 자르기\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = str(text).strip()\n",
        "    if len(text) <= max_chars:\n",
        "        return text\n",
        "    # max_chars까지만 보고 문장 종료부 찾기\n",
        "    cutoff = text[:max_chars]\n",
        "    match = re.findall(r'.*?[.?!]', cutoff)  # 마침표/물음표/느낌표까지 포함\n",
        "    if match:\n",
        "        return \"\".join(match).strip()\n",
        "    else:\n",
        "        return cutoff.strip()\n",
        "\n",
        "def prepare_corpus_embeddings(embeddings, device=None, pre_normalize=True):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    embs_t = torch.as_tensor(embeddings, device=device, dtype=torch.float32)\n",
        "    return F.normalize(embs_t, p=2, dim=1) if pre_normalize else embs_t\n",
        "\n",
        "def get_context_for_question(\n",
        "    text,\n",
        "    df,\n",
        "    embeddings,                 # torch.Tensor 또는 np.ndarray (가능하면 미리 prepare_corpus_embeddings로 정규화)\n",
        "    model_embedding,\n",
        "    top_n=3,\n",
        "    min_score=0.30,\n",
        "    max_chars=700,\n",
        "    pool_k=None,                # 상위 풀 크기 (None이면 자동)\n",
        "    lambda_mmr=0.7,             # MMR 가중\n",
        "    dedup_sim_thresh=0.95,      # 선택된 청크끼리 유사도 상한\n",
        "    source_col='source_type',   # 'law' / 'dictionary' 같은 컬럼명 (없으면 무시)\n",
        "    source_bias=None,           # 예: {'dictionary': 0.15, 'law': 0.0}\n",
        "    embeddings_pre_normalized=True,  # embeddings가 이미 L2 정규화 되었는지\n",
        "):\n",
        "    device = embeddings.device if torch.is_tensor(embeddings) else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 1) 코퍼스 임베딩 텐서 준비\n",
        "    if isinstance(embeddings, np.ndarray):\n",
        "        embs_t = torch.tensor(embeddings, device=device, dtype=torch.float32)\n",
        "    else:\n",
        "        embs_t = embeddings.to(device=device, dtype=torch.float32)\n",
        "\n",
        "    if not embeddings_pre_normalized:\n",
        "        embs_t = F.normalize(embs_t, p=2, dim=1)  # 쿼리마다 하지 말고 가능하면 외부에서 1회만\n",
        "\n",
        "    # 2) 쿼리 임베딩 (그대로 OK)\n",
        "    with torch.no_grad():\n",
        "        q = model_embedding.encode(text, convert_to_tensor=True, normalize_embeddings=True)\n",
        "        if q.device != device:\n",
        "            q = q.to(device)\n",
        "        scores = embs_t @ q  # (N,)\n",
        "\n",
        "    N = scores.numel()\n",
        "    if pool_k is None:\n",
        "        pool_k = min(max(top_n * 10, 50), N)\n",
        "\n",
        "    vals, idxs = torch.topk(scores, k=min(pool_k, N))\n",
        "    # vals = vals.clone()  # 굳이 필요 없음\n",
        "\n",
        "    # 3) 소스 가중치는 여기서 'pool_list' 만들 때만 1회 반영 (중복 가산 제거)\n",
        "    pool_list = []\n",
        "    use_bias = bool(source_bias) and (source_col in df.columns)\n",
        "    for s, i in zip(vals.tolist(), idxs.tolist()):\n",
        "        score = float(s)\n",
        "        if use_bias:\n",
        "            score += float(source_bias.get(str(df.iloc[int(i)][source_col]), 0.0))\n",
        "        pool_list.append((score, int(i)))\n",
        "\n",
        "    # 4) MMR로 선택(중복 억제)\n",
        "    selected_idxs = []\n",
        "    selected_scores = []\n",
        "\n",
        "    while len(selected_idxs) < min(top_n, len(pool_list)):\n",
        "        best_score, best_i = None, None\n",
        "        for s, i in pool_list:\n",
        "            if i in selected_idxs:\n",
        "                continue\n",
        "            if selected_idxs:\n",
        "                # 정규화 되어 있으니 내적으로 빠르게\n",
        "                div = max(float((embs_t[i] @ embs_t[j]).item()) for j in selected_idxs)\n",
        "            else:\n",
        "                div = 0.0\n",
        "            mmr = lambda_mmr * s - (1 - lambda_mmr) * div\n",
        "            if (best_score is None) or (mmr > best_score):\n",
        "                best_score, best_i = mmr, i\n",
        "\n",
        "        if best_i is None:\n",
        "            break\n",
        "\n",
        "        base_sim = float((embs_t[best_i] @ q).item())\n",
        "        if base_sim < min_score and len(selected_idxs) >= 1:\n",
        "            # 후보 하나 제거하고 계속\n",
        "            pool_list = [(s, i) for (s, i) in pool_list if i != best_i]\n",
        "            continue\n",
        "\n",
        "        # 선택 집합과 너무 유사하면 스킵\n",
        "        if dedup_sim_thresh and selected_idxs:\n",
        "            max_sim = max(float((embs_t[best_i] @ embs_t[j]).item()) for j in selected_idxs)\n",
        "            if max_sim >= dedup_sim_thresh:\n",
        "                pool_list = [(s, i) for (s, i) in pool_list if i != best_i]\n",
        "                continue\n",
        "\n",
        "        selected_idxs.append(best_i)\n",
        "        selected_scores.append(base_sim)\n",
        "        pool_list = [(s, i) for (s, i) in pool_list if i != best_i]\n",
        "\n",
        "    # 5) 부족하면 임계치 무시하고 보충 (점수 내림차순으로)\n",
        "    if len(selected_idxs) < top_n and pool_list:\n",
        "        for _, i in sorted(pool_list, key=lambda x: x[0], reverse=True):\n",
        "            if i in selected_idxs:\n",
        "                continue\n",
        "            selected_idxs.append(i)\n",
        "            selected_scores.append(float((embs_t[i] @ q).item()))\n",
        "            if len(selected_idxs) >= top_n:\n",
        "                break\n",
        "\n",
        "    # 6) 컨텍스트 구성(문장 경계로 트림)\n",
        "    contexts = []\n",
        "    for i in selected_idxs:\n",
        "        chunk = str(df.iloc[i]['chunk'])\n",
        "        per_limit = max_chars\n",
        "        if source_col in df.columns and max_chars:\n",
        "            st = str(df.iloc[i][source_col])\n",
        "            if st == 'law':\n",
        "                per_limit = int(max_chars * 1.6)\n",
        "            elif st == 'dictionary':\n",
        "                per_limit = max_chars\n",
        "        trimmed = _trim_to_sentence(chunk, per_limit)\n",
        "        if trimmed:\n",
        "            contexts.append(trimmed)\n",
        "\n",
        "    # ✅ 어떤 상황에서도 리스트 반환 보장\n",
        "    return contexts if isinstance(contexts, list) else []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPA2e-ZKxRO3"
      },
      "outputs": [],
      "source": [
        "def make_prompt_auto(text, df, embeddings, model_embedding, top_n=3):\n",
        "    # 컨텍스트 검색\n",
        "    context_chunks = get_context_for_question(text, df, embeddings, model_embedding, top_n=top_n) or []\n",
        "    context_text = \"\\n\\n\".join([f\"{idx+1}. {chunk}\" for idx, chunk in enumerate(context_chunks)]) if context_chunks else \"(없음)\"\n",
        "\n",
        "    if is_multiple_choice(text):\n",
        "        # 객관식 파싱\n",
        "        question, options = extract_question_and_choices(text)\n",
        "\n",
        "        # 옵션 전처리\n",
        "        options = [opt for opt in (opt.strip() for opt in options) if opt]\n",
        "        option_text = \"\\n\".join([f\"{idx}. {opt}\" for idx, opt in enumerate(options, start=1)])\n",
        "\n",
        "        prompt = (\n",
        "            \"[참고 정보]\\n\"\n",
        "            f\"{context_text}\\n\\n\"\n",
        "            \"당신은 금융보안 전문가입니다.\\n\"\n",
        "            \"참고 정보를 참고하여 당신의 지식을 기반으로 답하세요.\\n\"\n",
        "            \"아래 질문에 대해 근거를 제시하고, **정답 선택지 번호만 출력**하세요.\\n\\n\"\n",
        "            f\"질문: {question}\\n\"\n",
        "            f\"선택지:\\n{option_text}\\n\\n\"\n",
        "            \"정답 번호:\"\n",
        "        )\n",
        "    else:\n",
        "        prompt = (\n",
        "            \"[참고 정보]\\n\"\n",
        "            f\"{context_text}\\n\\n\"\n",
        "            f\"질문: {text}\\n\\n\"\n",
        "            \"당신은 금융보안 전문가입니다.\\n\"\n",
        "            \"참고 정보를 참고하여 당신의 지식을 기반으로 답하세요.\\n\"\n",
        "            \"주관식 질문에 대해 정확하고 간결하게 작성하세요.\\n\"\n",
        "            \"답변을 반복하지 마세요. \\n\"\n",
        "            \"3문장으로 답변한 후, [답변종료].\\n\\n\"\n",
        "            \"3문장 답변: \"\n",
        "        )\n",
        "\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "9PJF6YNqxSNq",
        "outputId": "7e3ea70e-a17e-45e5-e18d-fc7e4f50618d"
      },
      "outputs": [],
      "source": [
        "!pip install -qU transformers accelerate bitsandbytes\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "model_name = \"kakaocorp/kanana-1.5-8b-instruct-2505\"  # 또는 \"kakaocorp/kanana-1.5-8b-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,             # VRAM 아끼기\n",
        "    torch_dtype=torch.float16,     # 4bit면 fp16 권장\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 테스트\n",
        "out = pipe(\"안녕! 너는 누구야?\", max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.9)\n",
        "print(out[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_VEJman2g8U"
      },
      "outputs": [],
      "source": [
        "# ====== 동그라미 숫자 & 문자 매핑 ======\n",
        "CIRCLED_MAP = {\n",
        "    \"①\": \"1\", \"②\": \"2\", \"③\": \"3\", \"④\": \"4\", \"⑤\": \"5\",\n",
        "    \"⑥\": \"6\", \"⑦\": \"7\", \"⑧\": \"8\", \"⑨\": \"9\", \"⑩\": \"10\",\n",
        "    \"⑪\": \"11\", \"⑫\": \"12\", \"⑬\": \"13\", \"⑭\": \"14\", \"⑮\": \"15\",\n",
        "    \"⑯\": \"16\", \"⑰\": \"17\", \"⑱\": \"18\", \"⑲\": \"19\", \"⑳\": \"20\",\n",
        "}\n",
        "LETTER_MAP = {ch: str(i) for i, ch in enumerate(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", start=1)}\n",
        "\n",
        "# ====== 주관식 정제기 ======\n",
        "def _normalize_for_compare(s: str) -> str:\n",
        "    s = re.sub(r'\\s+', '', (s or \"\").lower())\n",
        "    s = re.sub(r'[^\\w가-힣]', '', s)\n",
        "    return s\n",
        "\n",
        "def _is_echo(line: str, original_question: str) -> bool:\n",
        "    # 🔧 None 가드\n",
        "    if not original_question:\n",
        "        return False\n",
        "    a = _normalize_for_compare(line)\n",
        "    b = _normalize_for_compare(original_question)\n",
        "    if not a or not b:\n",
        "        return False\n",
        "    sa, sb = set(a), set(b)\n",
        "    inter = len(sa & sb); union = len(sa | sb) or 1\n",
        "    return (inter/union) >= 0.8  # 임계치는 상황 보며 0.7~0.85 조절\n",
        "\n",
        "def _dedup_sentences(s: str) -> str:\n",
        "    parts = re.split(r'(?<=[.!?。])\\s+|\\n+', (s or \"\").strip())\n",
        "    seen, out = set(), []\n",
        "    for p in parts:\n",
        "        k = _normalize_for_compare(p)\n",
        "        if k and k not in seen:\n",
        "            seen.add(k)\n",
        "            out.append(p.strip())\n",
        "    return \" \".join(out).strip()\n",
        "\n",
        "# --- 일반 토크나이저(언어/도메인 비특화) ---\n",
        "def _tok(s: str):\n",
        "    return set(map(str.lower, re.findall(r'[A-Za-z가-힣0-9]{2,}', s or '')))\n",
        "\n",
        "def _has_new_tokens(line: str, question: str, min_new: int = 1) -> bool:\n",
        "    if not question:\n",
        "        return True\n",
        "    return len(_tok(line) - _tok(question)) >= min_new\n",
        "\n",
        "_INSTR_END_RE = re.compile(r'(세요|시오|하라)[\\.\\s]*$')\n",
        "def _is_instructional_line(line: str, original_question: str) -> bool:\n",
        "    if not _has_new_tokens(line, original_question, min_new=1):\n",
        "        return True\n",
        "    if _INSTR_END_RE.search((line or \"\").strip()):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _extract_subjective_answer(text: str, original_question: str = None,\n",
        "                               joiner: str = \", \", max_sentences: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    주관식: 첫 유효 줄 + 그 아래 목록형(①, 1), (1), A., A) 등) 라인들을 모두 합쳐 반환.\n",
        "    목록이 없으면, 에코/명령형 제외 후 앞에서부터 최대 N문장 보충.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"미응답\"\n",
        "\n",
        "    # ✅ [답변종료] 이전까지만 사용 (대소문자/공백 허용)\n",
        "    text = re.split(r'\\[ *답변\\s*종료[^\\]]*\\]', text, maxsplit=1)[0]\n",
        "\n",
        "    t = text.strip()\n",
        "    t = re.sub(r'^\\s*(정답|답안|답|Answer|답변)\\s*[:：\\-]\\s*', '', t, flags=re.I)\n",
        "    t = t.strip('\"\\'' \"“”‘’`\")\n",
        "    t = re.sub(r'^\\s*([\\-–—•·]\\s*)+', '', t)\n",
        "\n",
        "    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]\n",
        "    if not lines:\n",
        "        return \"미응답\"\n",
        "\n",
        "    # 1) 첫 유효 줄\n",
        "    first_idx = None\n",
        "    for i, ln in enumerate(lines):\n",
        "        if _is_echo(ln, original_question):\n",
        "            continue\n",
        "        if _is_instructional_line(ln, original_question):\n",
        "            continue\n",
        "        first_idx = i\n",
        "        break\n",
        "    if first_idx is None:\n",
        "        return \"미응답\"\n",
        "\n",
        "    collected = [lines[first_idx]]\n",
        "\n",
        "    # 2) 뒤에 연속된 목록형 라인 수집\n",
        "    j = first_idx + 1\n",
        "    took_list = False\n",
        "    while j < len(lines) and OPTION_LINE_RE.match(lines[j]):\n",
        "        collected.append(strip_option_prefix(lines[j]))\n",
        "        took_list = True\n",
        "        j += 1\n",
        "\n",
        "    # 3) 목록이 없으면 뒤쪽 문장 보충\n",
        "    if not took_list:\n",
        "        tail = \" \".join(lines[first_idx+1:])\n",
        "        sents = [s.strip() for s in re.split(r'(?<=[.!?。])\\s+|\\n+', tail) if s.strip()]\n",
        "        picked = []\n",
        "        for s in sents:\n",
        "            if _is_echo(s, original_question):\n",
        "                continue\n",
        "            if _is_instructional_line(s, original_question):\n",
        "                continue\n",
        "            if any(_normalize_for_compare(s) == _normalize_for_compare(p) for p in picked):\n",
        "                continue\n",
        "            picked.append(s)\n",
        "            if len(picked) >= max_sentences:\n",
        "                break\n",
        "        collected.extend(picked)\n",
        "\n",
        "    out = joiner.join(collected)\n",
        "    out = _dedup_sentences(out)\n",
        "    out = re.sub(r'(.)\\1{3,}', r'\\1\\1', out)\n",
        "    return out[:500] if out else \"미응답\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_answer_only(generated_text: str, original_question: str) -> str:\n",
        "    \"\"\"\n",
        "    LLM 출력에서 '정답만' 추출\n",
        "      - 주관식: _extract_subjective_answer 사용\n",
        "      - 객관식: 숫자/문자/원형숫자/`n번` 패턴 등 탐색\n",
        "    반환:\n",
        "      - 객관식: \"1\"~\"20\" (검출 실패 시 \"0\")\n",
        "      - 주관식: 자유 텍스트(최대 300자), 없으면 \"미응답\"\n",
        "    \"\"\"\n",
        "    if not isinstance(generated_text, str) or not generated_text.strip():\n",
        "        return \"미응답\"\n",
        "\n",
        "    text = generated_text\n",
        "\n",
        "    # \"답변:\" / \"Answer:\" 이후만 사용 (대소문자 구분 없음)\n",
        "    m = re.search(r'(답변|Answer)\\s*[:：\\-]\\s*', text, flags=re.I)\n",
        "    if m:\n",
        "        text = text[m.end():]\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return \"미응답\"\n",
        "\n",
        "    # 주/객관식 분기\n",
        "    if not is_multiple_choice(original_question):\n",
        "        return _extract_subjective_answer(text, original_question=original_question)\n",
        "\n",
        "    # ===== 객관식 처리 =====\n",
        "    num_opts = count_options(original_question)  # 선택지 개수(검증용, 0이면 검증 생략)\n",
        "\n",
        "    # 0) 동그라미 숫자 정규화\n",
        "    for k, v in CIRCLED_MAP.items():\n",
        "        if k in text:\n",
        "            text = text.replace(k, v)\n",
        "\n",
        "    # 1) \"정답 번호: x\" / \"정답: x\"\n",
        "    m = re.search(r\"정답\\s*(?:번호)?\\s*[:：]?\\s*([1-9][0-9]?)(?:\\s*번)?\\b\", text)\n",
        "    if m:\n",
        "        ans = m.group(1)\n",
        "        return ans if (not num_opts or int(ans) <= num_opts) else \"0\"\n",
        "\n",
        "    # 1-2) \"정답: A\" (문자 → 숫자)\n",
        "    m = re.search(r\"정답\\s*[:：]?\\s*([A-Za-z])\\b\", text)\n",
        "    if m:\n",
        "        cand = LETTER_MAP.get(m.group(1).upper(), \"0\")\n",
        "        return cand if (not num_opts or int(cand) <= num_opts) else \"0\"\n",
        "\n",
        "    # 2) 숫자만 있는 단일 라인\n",
        "    for ln in text.splitlines():\n",
        "        ln = ln.strip()\n",
        "        if re.fullmatch(r\"[1-9][0-9]?\", ln):\n",
        "            return ln if (not num_opts or int(ln) <= num_opts) else \"0\"\n",
        "\n",
        "    # 3) 선지(보기) 라인은 제거하고 다시 탐색\n",
        "    cleaned_lines = []\n",
        "    for ln in text.splitlines():\n",
        "        s = ln.strip()\n",
        "        if OPTION_LINE_RE.match(s):\n",
        "            continue\n",
        "        cleaned_lines.append(s)\n",
        "    s = \" \".join(cleaned_lines)\n",
        "\n",
        "    # 4) 단독 문자(A/B/C/...) 있으면 숫자로 변환\n",
        "    m = re.search(r\"\\b([A-Za-z])\\b\", s)\n",
        "    if m:\n",
        "        cand = LETTER_MAP.get(m.group(1).upper(), \"0\")\n",
        "        return cand if (not num_opts or int(cand) <= num_opts) else \"0\"\n",
        "\n",
        "    # 5) \"3번\" / 일반 숫자\n",
        "    m = re.search(r\"\\b([1-9][0-9]?)\\s*번\\b\", s)\n",
        "    if m:\n",
        "        ans = m.group(1)\n",
        "        return ans if (not num_opts or int(ans) <= num_opts) else \"0\"\n",
        "\n",
        "    m = re.search(r\"\\b([1-9][0-9]?)\\b\", s)\n",
        "    if m:\n",
        "        ans = m.group(1)\n",
        "        return ans if (not num_opts or int(ans) <= num_opts) else \"0\"\n",
        "\n",
        "    # 6) 실패\n",
        "    return \"0\"\n"
      ],
      "metadata": {
        "id": "0fkW01Jf0cgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# q = \"\"\"관리체계 수립 및 운영'의 '정책 수립' 단계에서 가장 중요한 요소는 무엇인가?\n",
        "# 1 정보보호 및 개인정보보호 정책의 제·개정\n",
        "# 2 경영진의 참여\n",
        "# 3 최고책임자의 지정\n",
        "# 4 자원 할당\n",
        "# 5 내부 감사 절차의 수립\n",
        "# \"\"\"\n",
        "# print(is_multiple_choice(q))  # True\n",
        "# print(extract_question_and_choices(q))\n",
        "# generated = \"정답: 2번입니다. 경영진의 참여가 핵심...\"\n",
        "# print(extract_answer_only(generated, q))  # \"2\"\n",
        "\n",
        "# q2 = \"전자금융거래법 제47조의 취지를 간단히 설명하시오.\"\n",
        "# generated2 = \"답변: 전자금융거래 현황 파악과 통화신용정책 수립을 위한 통계조사를 가능하게 합니다.\"\n",
        "# print(is_multiple_choice(q2))  # False\n",
        "# print(extract_answer_only(generated2, q2))\n"
      ],
      "metadata": {
        "id": "Su1jg2ODzx5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1) 데이터 로드\n",
        "law_df  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/법령고시/금융보안_법률_데이터_전처리_최종.csv')\n",
        "dict_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/csrc_glossary_ko_cleaned_1.csv')\n",
        "econ_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/시사경제용어사전.csv')\n",
        "ICT_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/merged_ICT_words.csv')\n",
        "\n",
        "# 2) 법령 chunk 만들기\n",
        "law_df['chunk'] = (\n",
        "    law_df['법령명'].astype(str) + ' ' +\n",
        "    law_df['조문헤더'].astype(str) + ' ' +\n",
        "    law_df['항내용'].astype(str)\n",
        ")\n",
        "law_df['source_type'] = 'law'\n",
        "\n",
        "# 3 -a) 용어사전 chunk 만들기\n",
        "dict_df = dict_df.fillna('')\n",
        "dict_df['chunk'] = (\n",
        "    dict_df['term'].astype(str) + ' ' +\n",
        "    dict_df['aliases'].astype(str) + ' ' +\n",
        "    dict_df['definition_ko'].astype(str)\n",
        ")\n",
        "dict_df['source_type'] = 'dictionary'\n",
        "\n",
        "# 3-b) 시사경제용어사전 chunk 만들기 (용어 + 설명)\n",
        "econ_df = econ_df.fillna('')\n",
        "econ_df['chunk'] = (\n",
        "    econ_df['용어'].astype(str) + ' ' +\n",
        "    econ_df['설명'].astype(str)\n",
        ")\n",
        "econ_df['source_type'] = 'econ_dictionary'\n",
        "\n",
        "# 3-c) ICT_df chunk 만들기\n",
        "ICT_df = ICT_df.fillna('')\n",
        "\n",
        "ko = ICT_df['국문표제어'].astype(str).str.strip()\n",
        "en = ICT_df['영문표제어'].astype(str).str.strip()\n",
        "ab = ICT_df['약어'].astype(str).str.strip()\n",
        "body = ICT_df['전처리'].astype(str).str.strip()\n",
        "\n",
        "# 영문 표제어는 괄호 안에 넣기\n",
        "title = np.where(en != '', ko + ' (' + en + ')', ko)\n",
        "\n",
        "# 약어가 있으면 , (약어) 붙이기\n",
        "abbr = np.where(ab != '', ', (' + ab + ')', '')\n",
        "\n",
        "ICT_df['chunk'] = (title + abbr + ' ' + body)\\\n",
        "    .str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "ICT_df['source_type'] = 'ict_dictionary'\n",
        "\n",
        "# 4) 통합\n",
        "merged_df = pd.concat(\n",
        "    [law_df[['chunk','source_type']],\n",
        "    #  dict_df[['chunk','source_type']],\n",
        "     econ_df[['chunk','source_type']],\n",
        "     ICT_df[['chunk','source_type']]],\n",
        "    ignore_index=True\n",
        ").drop_duplicates(subset=['chunk']).reset_index(drop=True)\n",
        "\n",
        "# 5) 임베딩 모델 로드\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_embedding = SentenceTransformer('jhgan/ko-sroberta-multitask', device=device)\n",
        "\n",
        "# 6) 전체 chunk 임베딩 생성\n",
        "embeddings = model_embedding.encode(\n",
        "    merged_df['chunk'].tolist(),\n",
        "    batch_size=64,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True  # 코사인 유사도 계산 편하게 정규화\n",
        ")\n",
        "\n",
        "# 7) 길이 검증\n",
        "print(\"merged_df 길이:\", len(merged_df))\n",
        "print(\"embeddings 길이:\", embeddings.shape[0])\n",
        "\n",
        "# (선택) 저장\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/merged_embeddings.npy', embeddings)\n",
        "merged_df.to_csv('/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/merged_chunks.csv', index=False)\n"
      ],
      "metadata": {
        "id": "pNuDgXrcGbOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08b81a11-7825-4bb5-f4a9-b9120f868bff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "corpus_embeddings = torch.from_numpy(\n",
        "    np.load('/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/merged_embeddings.npy', allow_pickle=True)\n",
        ").float().to('cuda')"
      ],
      "metadata": {
        "id": "n9lNavc7kslP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "tjCUJ2ydyn0a",
        "outputId": "7e511ea4-29ff-4963-f41e-dca15fddc135"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm\n",
        "\n",
        "# # ✅ 주관식만 필터링\n",
        "# subjective_df = test.loc[~test['Question'].apply(is_multiple_choice)].copy()\n",
        "\n",
        "# preds_subjective = []\n",
        "\n",
        "# for q in tqdm(subjective_df['Question'], desc=\"Inference (주관식만)\"):\n",
        "#     print(\"=\" * 80)\n",
        "#     print(f\"[Question]\\n{q}\")\n",
        "\n",
        "#     # 1) 프롬프트 생성\n",
        "#     prompt = make_prompt_auto(q, merged_df, corpus_embeddings, model_embedding, top_n=3)\n",
        "#     print(\"-\" * 80)\n",
        "#     print(f\"[Prompt Sent to Model]\\n{prompt}\")\n",
        "\n",
        "#     # 2) 모델 추론\n",
        "#     output = pipe(prompt, max_new_tokens=256, temperature=0.2, top_p=0.9)\n",
        "#     raw_output = output[0][\"generated_text\"]\n",
        "#     print(\"-\" * 80)\n",
        "#     print(f\"[Raw Model Output]\\n{raw_output}\")\n",
        "\n",
        "#     # 3) 정답 추출\n",
        "#     pred_answer = extract_answer_only(raw_output, original_question=q)\n",
        "#     print(\"-\" * 80)\n",
        "#     print(f\"[Extracted Final Answer] -> {pred_answer}\")\n",
        "\n",
        "#     preds_subjective.append(pred_answer)\n",
        "\n",
        "# # 결과 저장\n",
        "# subjective_df['Predicted Answer'] = preds_subjective\n"
      ],
      "metadata": {
        "id": "MY_oU594wc4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm\n",
        "\n",
        "# # ✅ 객관식만 필터링\n",
        "# subjective_df = test.loc[test['Question'].apply(is_multiple_choice)].copy()\n",
        "\n",
        "# preds_subjective = []\n",
        "\n",
        "# for q in tqdm(subjective_df['Question'], desc=\"Inference (객관식만)\"):\n",
        "#     print(\"=\" * 80)\n",
        "#     print(f\"[Question]\\n{q}\")\n",
        "\n",
        "#     # 1) 프롬프트 생성\n",
        "#     prompt = make_prompt_auto(q, merged_df, corpus_embeddings, model_embedding, top_n=3)\n",
        "#     print(\"-\" * 80)\n",
        "#     print(f\"[Prompt Sent to Model]\\n{prompt}\")\n",
        "\n",
        "#     # 2) 모델 추론\n",
        "#     output = pipe(prompt, max_new_tokens=64, temperature=0.2, top_p=0.9)\n",
        "#     raw_output = output[0][\"generated_text\"]\n",
        "#     print(\"-\" * 80)\n",
        "#     print(f\"[Raw Model Output]\\n{raw_output}\")\n",
        "\n",
        "#     # 3) 정답 추출\n",
        "#     pred_answer = extract_answer_only(raw_output, original_question=q)\n",
        "#     print(\"-\" * 80)\n",
        "#     print(f\"[Extracted Final Answer] -> {pred_answer}\")\n",
        "\n",
        "#     preds_subjective.append(pred_answer)\n",
        "\n",
        "# # 결과 저장\n",
        "# subjective_df['Predicted Answer'] = preds_subjective\n"
      ],
      "metadata": {
        "id": "2gmd9GrAF-9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preds = []\n",
        "\n",
        "# for q in tqdm(test['Question'], desc=\"Inference\"):\n",
        "#     prompt = make_prompt_auto(q, merged_df, corpus_embeddings, model_embedding, top_n=2)  # ✅ corpus_embeddings 사용\n",
        "#     print(\"=\" * 80)\n",
        "#     print(f\"Q: {q}\")\n",
        "#     print(\"\\n[Prompt]\\n\" + prompt)\n",
        "#     print(\"-\" * 80)\n",
        "\n",
        "#     output = pipe(prompt, max_new_tokens=128, temperature=0.2, top_p=0.9)\n",
        "#     print(\"\\n[Raw Model Output]\\n\", output[0][\"generated_text\"])\n",
        "\n",
        "#     pred_answer = extract_answer_only(output[0][\"generated_text\"], original_question=q)\n",
        "#     print(\"\\n[Extracted Answer] ->\", pred_answer)\n",
        "\n",
        "#     preds.append(pred_answer)\n"
      ],
      "metadata": {
        "id": "5FjNEQXjDD13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 파인튜닝"
      ],
      "metadata": {
        "id": "CVVsevLqv4hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -U transformers accelerate datasets peft"
      ],
      "metadata": {
        "id": "DZunxJLLz0OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import DatasetDict, Dataset\n",
        "# from transformers import TrainingArguments, Trainer\n",
        "# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "# import bitsandbytes as bnb\n",
        "# import json\n",
        "\n",
        "# TRAIN = \"/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/파인튜닝 데이터셋/mcq_train.jsonl\"\n",
        "# VAL   = \"/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/파인튜닝 데이터셋/mcq_val.jsonl\"\n",
        "# BASE  = model_name\n",
        "\n",
        "# def load_jsonl_rows(path):\n",
        "#     with open(path,\"r\",encoding=\"utf-8\") as f:\n",
        "#         return [json.loads(x) for x in f]\n",
        "\n",
        "# train_rows = load_jsonl_rows(TRAIN)\n",
        "# val_rows   = load_jsonl_rows(VAL)\n",
        "\n",
        "# def build_pair(ex):\n",
        "#     # 프롬프트 고정 (튜닝 시에도 “정답 번호만” 생성하도록)\n",
        "#     prompt = \"다음 객관식 문제의 정답 번호만 출력하라. 설명 금지.\\n\\n\" + ex[\"input\"] + \"\\n\\n정답 번호:\"\n",
        "#     completion = \" \" + ex[\"output\"]  # 앞 공백 한 칸 두면 학습 안정적\n",
        "#     return {\"prompt\": prompt, \"completion\": completion}\n",
        "\n",
        "# train_pairs = list(map(build_pair, train_rows))\n",
        "# val_pairs   = list(map(build_pair, val_rows))\n",
        "# ds = DatasetDict({\n",
        "#     \"train\": Dataset.from_list(train_pairs),\n",
        "#     \"validation\": Dataset.from_list(val_pairs)\n",
        "# })\n",
        "\n",
        "# tok = AutoTokenizer.from_pretrained(BASE, use_fast=True, trust_remote_code=True)\n",
        "# tok.padding_side = \"left\"\n",
        "# if tok.pad_token_id is None:\n",
        "#     tok.pad_token = tok.eos_token or \"</s>\"\n",
        "\n",
        "# MAX_LEN = 640\n",
        "# IGNORE_INDEX = -100\n",
        "\n",
        "# def tok_mask(batch):\n",
        "#     p = tok(batch[\"prompt\"], add_special_tokens=False)\n",
        "#     c = tok(batch[\"completion\"], add_special_tokens=False)\n",
        "#     input_ids, attn, labels = [], [], []\n",
        "#     for p_ids, c_ids in zip(p[\"input_ids\"], c[\"input_ids\"]):\n",
        "#         max_c = MAX_LEN - len(p_ids)\n",
        "#         if max_c < 1:\n",
        "#             p_ids = p_ids[-(MAX_LEN-1):]\n",
        "#             max_c = 1\n",
        "#         if len(c_ids) > max_c:\n",
        "#             c_ids = c_ids[:max_c]\n",
        "#         ids = p_ids + c_ids\n",
        "#         am  = [1]*len(ids)\n",
        "#         if len(ids) < MAX_LEN:\n",
        "#             pad = MAX_LEN - len(ids)\n",
        "#             ids += [tok.pad_token_id]*pad\n",
        "#             am  += [0]*pad\n",
        "#         lab = [IGNORE_INDEX]*len(p_ids) + c_ids\n",
        "#         if len(lab) < MAX_LEN:\n",
        "#             lab += [IGNORE_INDEX]*(MAX_LEN-len(lab))\n",
        "#         input_ids.append(ids); attn.append(am); labels.append(lab)\n",
        "#     return {\"input_ids\": input_ids, \"attention_mask\": attn, \"labels\": labels}\n",
        "\n",
        "# tok_ds = ds.map(tok_mask, batched=True, remove_columns=[\"prompt\",\"completion\"])\n",
        "\n",
        "# # QLoRA (A100: 4bit + bfloat16 OK)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     BASE, load_in_4bit=True, device_map=\"auto\",\n",
        "#     torch_dtype=torch.bfloat16, trust_remote_code=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True\n",
        "# )\n",
        "# model = prepare_model_for_kbit_training(model)\n",
        "# lora_cfg = LoraConfig(\n",
        "#     r=16, lora_alpha=16, lora_dropout=0.05,\n",
        "#     target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        "# )\n",
        "# model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "# from transformers import TrainingArguments\n",
        "\n",
        "# args = TrainingArguments(\n",
        "#     output_dir=\"/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/파인튜닝 데이터셋\",\n",
        "#     per_device_train_batch_size=1,\n",
        "#     gradient_accumulation_steps=4,\n",
        "#     num_train_epochs=1,\n",
        "#     learning_rate=2e-5,\n",
        "#     bf16=True,\n",
        "#     logging_steps=10,\n",
        "#     save_steps=100,\n",
        "#     do_eval=True,\n",
        "#     eval_steps=100,\n",
        "#     save_total_limit=2,\n",
        "#     report_to=\"none\"\n",
        "# )\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#     import numpy as np\n",
        "#     preds, labels = eval_pred\n",
        "#     # labels에서 프롬프트(-100) 제외하고 첫 토큰만 확인\n",
        "#     # (정답 한 글자이므로 첫 실제 라벨 토큰과 비교)\n",
        "#     label_ids = []\n",
        "#     for row in labels:\n",
        "#         row = [x for x in row if x != IGNORE_INDEX]\n",
        "#         label_ids.append(row[0] if row else -1)\n",
        "#     # preds에서 가장 확률 높은 토큰 뽑기\n",
        "#     pred_ids = preds.argmax(-1)\n",
        "#     # 첫 생성 토큰만 비교\n",
        "#     hits = 0; total = 0\n",
        "#     for p, l in zip(pred_ids, label_ids):\n",
        "#         if l == -1:\n",
        "#             continue\n",
        "#         total += 1\n",
        "#         if p == l:\n",
        "#             hits += 1\n",
        "#     return {\"em_token\": hits/total if total else 0.0}\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model, args=args,\n",
        "#     train_dataset=tok_ds[\"train\"],\n",
        "#     eval_dataset=tok_ds[\"validation\"],\n",
        "#     tokenizer=tok,\n",
        "#     compute_metrics=compute_metrics\n",
        "# )\n",
        "\n",
        "# trainer.train()\n",
        "# trainer.save_model(\"/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/파인튜닝 데이터셋\")\n"
      ],
      "metadata": {
        "id": "zbWmhqegv6_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer, pipeline\n",
        "# from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "# ADAPTER_DIR = \"/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/파인튜닝 데이터셋/checkpoint-2777\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR, use_fast=True, trust_remote_code=True)\n",
        "# tokenizer.padding_side = \"left\"\n",
        "# if tokenizer.pad_token_id is None:\n",
        "#     tokenizer.pad_token = tokenizer.eos_token or \"</s>\"\n",
        "\n",
        "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "#     ADAPTER_DIR, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True\n",
        "# ).eval()\n"
      ],
      "metadata": {
        "id": "G82vGjmSv7cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5PB0PeXv7ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "\n",
        "# 두 개의 pipe 설정 (객관식 / 주관식)\n",
        "pipe_mc = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\",\n",
        "    # 객관식: 길이 짧게, temperature 낮게\n",
        "    max_new_tokens=64,\n",
        "    temperature=0.1,\n",
        "    top_p=0.9\n",
        ")\n",
        "pipe_short = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\",\n",
        "    # 주관식: 길이 길게, temperature 조금 높게\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.2,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "for q in tqdm(test['Question'], desc=\"Inference\"):\n",
        "    # 질문 구분\n",
        "    if is_multiple_choice(q):\n",
        "        used_pipe = pipe_mc\n",
        "    else:\n",
        "        used_pipe = pipe_short\n",
        "\n",
        "    prompt = make_prompt_auto(q, merged_df, embeddings, model_embedding, top_n=3)\n",
        "    output = used_pipe(prompt)\n",
        "    pred_answer = extract_answer_only(output[0][\"generated_text\"], original_question=q)\n",
        "    preds.append(pred_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC0NL4eTNVXl",
        "outputId": "3266c285-8c31-45d8-9d95-0537b035c101"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MR1QNKo6xTcI",
        "outputId": "42c1d403-fb50-4715-9212-1222a1296b24"
      },
      "outputs": [],
      "source": [
        "# preds = []\n",
        "\n",
        "# for q in tqdm(test['Question'], desc=\"Inference\"):\n",
        "#     prompt = make_prompt_auto(q, merged_df, embeddings, model_embedding, top_n=3)\n",
        "#     output = pipe(prompt, max_new_tokens=128, temperature=0.2, top_p=0.9)\n",
        "#     pred_answer = extract_answer_only(output[0][\"generated_text\"], original_question=q)\n",
        "#     preds.append(pred_answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "id": "T4F5GgiFjQs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8984282-2d17-4d6f-afc7-66f2a576e24c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYgzgrw42jJE"
      },
      "outputs": [],
      "source": [
        "sample_submission = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/sample_submission.csv')\n",
        "sample_submission['Answer'] = preds\n",
        "sample_submission.to_csv('/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/결과물/kanana_RAG_ver1.csv', index=False, encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKY9IEzvDHme"
      },
      "source": [
        "# 데이터 청크 LAG"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "# jhgan/ko-sroberta-multitask 이 모델이 코사인 유사도 결과가 더 잘나옴\n",
        "model_embedding = SentenceTransformer('jhgan/ko-sroberta-multitask')"
      ],
      "metadata": {
        "id": "p5S_nOUbD5SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldviv_8i4QyG",
        "outputId": "43c231bc-d02a-400a-d2d9-0e1b18ba79b3"
      },
      "outputs": [],
      "source": [
        "# SentenceTransformer 사용해서 벡터 임베딩 진행\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/법령_고시_데이터_전처리_3.csv')  # 파일 경로 맞게 수정\n",
        "\n",
        "# 법령명, 조문헤더, 본문 합치기\n",
        "df['chunk'] = df['법령명'].astype(str) + \" \" + df['조문헤더'].astype(str) + \" \" + df['본문'].astype(str)\n",
        "\n",
        "# 결과 확인\n",
        "print(df[['chunk']].head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#임베딩 모델 이미 설정 완료\n",
        "chunks = df['chunk'].tolist()\n",
        "embeddings = model_embedding.encode(chunks, show_progress_bar=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "G3rmIFNHouvk",
        "outputId": "75bb27ee-0b5c-4485-a5f4-9b88eb3bf1d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiAQRInA4ZNv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/chunk_embeddings_1.npy', embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df['법령명'].str.contains('개인정보 보호법')) & (df['조문헤더'].str.contains('제2조'))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "HhIIfGHesKz9",
        "outputId": "60826ef6-f01e-4a0c-9201-9c8626faa367"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eL-0QjW4pn3",
        "outputId": "d6545487-d755-4011-d7a6-8ee99765b462"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import util\n",
        "import numpy as np\n",
        "\n",
        "# 저장된 임베딩 불러오기\n",
        "embeddings = np.load('/content/drive/MyDrive/Colab Notebooks/금융공모전_25_8/chunk_embeddings_1.npy')\n",
        "chunks = df['chunk'].tolist()\n",
        "\n",
        "# 예시 쿼리\n",
        "query = \"개인정보 보호법이란?\"\n",
        "query_emb = model_embedding.encode(query)\n",
        "\n",
        "# 유사도 계산 (cos_scores shape: (N,))\n",
        "cos_scores = util.cos_sim(query_emb, embeddings)[0].cpu().numpy() if hasattr(util.cos_sim(query_emb, embeddings)[0], \"cpu\") else util.cos_sim(query_emb, embeddings)[0]\n",
        "top_n = 10\n",
        "\n",
        "# 유사도 내림차순 Top N 인덱스 추출\n",
        "top_idx = np.argsort(-cos_scores)[:top_n]   # <-- 여기서 바로 인덱스 생성!\n",
        "\n",
        "print(\"유사도가 높은 청크:\")\n",
        "for rank, idx in enumerate(top_idx, 1):\n",
        "    row = df.iloc[int(idx)]\n",
        "    print(f\"[{rank}] {row['법령명']} | {row['조문헤더']} | {row['본문'][:120]}... (score: {cos_scores[idx]:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmbSYKym4sV6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}