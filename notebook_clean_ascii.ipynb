{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aflw4PblpFZX"
      },
      "source": [
        "# Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnI8e0yg6e03",
        "outputId": "fea6cc3d-1a10-4540-cc9c-96889d93bbc4"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPeCIGQn6IgK",
        "outputId": "856a630c-aacf-4111-b7c8-508e961c903d"
      },
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUzdMrv2_4Ic",
        "outputId": "7e4f02a8-9a25-44b9-cef0-e79115220e8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p76vsXXwLL9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf1l405hwN6k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "e5833a4f-5d2b-4ed8-9529-2ab76c3b0bc8"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/test.csv')\n",
        "test #í”„ë¡œí† íƒ€ì…ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì‹¤í—˜ ì§„í–‰ìš© 50ê°œ ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO0MpuiwwsbQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# ====== ìˆ¨ì€ ë¬¸ì ì œê±° ======\n",
        "def norm_line(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    return (s.replace(\"\\xa0\", \" \")\n",
        "              .replace(\"\\u200b\", \"\")\n",
        "              .replace(\"\\u200c\", \"\")\n",
        "              .replace(\"\\u200d\", \"\")\n",
        "              .replace(\"\\ufeff\", \"\"))\n",
        "\n",
        "# ====== ê°ê´€ì‹ ì„ ì§€ íŒ¨í„´ ======\n",
        "OPTION_LINE_RE = re.compile(\n",
        "    r'^[\\s\\u200b\\u200c\\u200d\\ufeff]*'              # ì„ í–‰ ê³µë°±/ì œë¡œí­\n",
        "    r'(?:'\n",
        "    r'  \\(\\d{1,2}\\)'                               # (1) ~ (12)\n",
        "    r' | \\d{1}\\s+'                                 # 1â  ë‚´ìš© (1ìë¦¬+ê³µë°± í—ˆìš©)\n",
        "    r' | \\d{1,2}[.)]\\s*'                           # 1. / 1) / 12. / 12)\n",
        "    r' | [â‘ -â‘³]\\s*'                                 # â‘ ~â‘³\n",
        "    r' | [A-F][.)]\\s*'                              # A. / A) (ëŒ€ë¬¸ìë§Œ, ê³µë°± ë¶ˆí—ˆ)\n",
        "    r')',\n",
        "    re.X\n",
        ")\n",
        "\n",
        "def count_options(question_text: str) -> int:\n",
        "    \"\"\"\n",
        "    ì¤„ ë‹¨ìœ„ë¡œ ì„ ì§€ íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ” ë¼ì¸ ê°œìˆ˜\n",
        "    \"\"\"\n",
        "    cnt = 0\n",
        "    for ln in str(question_text).splitlines():\n",
        "        ln = norm_line(ln)\n",
        "        if OPTION_LINE_RE.match(ln):\n",
        "            cnt += 1\n",
        "    return cnt\n",
        "\n",
        "def is_multiple_choice(question_text: str) -> bool:\n",
        "    \"\"\"\n",
        "    ê°ê´€ì‹ ì—¬ë¶€ íŒë‹¨ (ê¸°ë³¸: ì„ ì§€ íŒ¨í„´ 2ê°œ ì´ìƒ)\n",
        "    \"\"\"\n",
        "    cnt = count_options(question_text)\n",
        "    return cnt >= 2\n",
        "\n",
        "def strip_option_prefix(line: str) -> str:\n",
        "    \"\"\"\n",
        "    '1) ë‚´ìš©', '(2) ë‚´ìš©', 'A. ë‚´ìš©', 'â‘  ë‚´ìš©'ì—ì„œ ì ‘ë‘ ê¸°í˜¸ ì œê±°\n",
        "    \"\"\"\n",
        "    m = OPTION_LINE_RE.match(line)\n",
        "    if not m:\n",
        "        return line.strip()\n",
        "    return line[m.end():].strip()\n",
        "\n",
        "def extract_question_and_choices(full_text: str):\n",
        "    lines = [norm_line(ln).rstrip() for ln in str(full_text).strip().split(\"\\n\")]\n",
        "    q_lines, options = [], []\n",
        "    for line in lines:\n",
        "        if OPTION_LINE_RE.match(line):\n",
        "            options.append(strip_option_prefix(line))\n",
        "        else:\n",
        "            q_lines.append(line.strip())\n",
        "    question = re.sub(r\"\\s+\", \" \", \" \".join([ln for ln in q_lines if ln]))\n",
        "    return question, options\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mV0X8A85y8B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sentence_transformers import util\n",
        "def _trim_to_sentence(text, max_chars):\n",
        "    \"\"\"\n",
        "    ìµœëŒ€ max_chars ê¸¸ì´ê¹Œì§€ ì˜ë¼ë‚´ë˜ ë¬¸ì¥ ê²½ê³„(ë§ˆì¹¨í‘œ, ê°œí–‰ ë“±) ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê¸°\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = str(text).strip()\n",
        "    if len(text) <= max_chars:\n",
        "        return text\n",
        "    # max_charsê¹Œì§€ë§Œ ë³´ê³  ë¬¸ì¥ ì¢…ë£Œë¶€ ì°¾ê¸°\n",
        "    cutoff = text[:max_chars]\n",
        "    match = re.findall(r'.*?[.?!]', cutoff)  # ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œê¹Œì§€ í¬í•¨\n",
        "    if match:\n",
        "        return \"\".join(match).strip()\n",
        "    else:\n",
        "        return cutoff.strip()\n",
        "\n",
        "def prepare_corpus_embeddings(embeddings, device=None, pre_normalize=True):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    embs_t = torch.as_tensor(embeddings, device=device, dtype=torch.float32)\n",
        "    return F.normalize(embs_t, p=2, dim=1) if pre_normalize else embs_t\n",
        "\n",
        "def get_context_for_question(\n",
        "    text,\n",
        "    df,\n",
        "    embeddings,                 # torch.Tensor ë˜ëŠ” np.ndarray (ê°€ëŠ¥í•˜ë©´ ë¯¸ë¦¬ prepare_corpus_embeddingsë¡œ ì •ê·œí™”)\n",
        "    model_embedding,\n",
        "    top_n=3,\n",
        "    min_score=0.30,\n",
        "    max_chars=700,\n",
        "    pool_k=None,                # ìƒìœ„ í’€ í¬ê¸° (Noneì´ë©´ ìë™)\n",
        "    lambda_mmr=0.7,             # MMR ê°€ì¤‘\n",
        "    dedup_sim_thresh=0.95,      # ì„ íƒëœ ì²­í¬ë¼ë¦¬ ìœ ì‚¬ë„ ìƒí•œ\n",
        "    source_col='source_type',   # 'law' / 'dictionary' ê°™ì€ ì»¬ëŸ¼ëª… (ì—†ìœ¼ë©´ ë¬´ì‹œ)\n",
        "    source_bias=None,           # ì˜ˆ: {'dictionary': 0.15, 'law': 0.0}\n",
        "    embeddings_pre_normalized=True,  # embeddingsê°€ ì´ë¯¸ L2 ì •ê·œí™” ë˜ì—ˆëŠ”ì§€\n",
        "):\n",
        "    device = embeddings.device if torch.is_tensor(embeddings) else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 1) ì½”í¼ìŠ¤ ì„ë² ë”© í…ì„œ ì¤€ë¹„\n",
        "    if isinstance(embeddings, np.ndarray):\n",
        "        embs_t = torch.tensor(embeddings, device=device, dtype=torch.float32)\n",
        "    else:\n",
        "        embs_t = embeddings.to(device=device, dtype=torch.float32)\n",
        "\n",
        "    if not embeddings_pre_normalized:\n",
        "        embs_t = F.normalize(embs_t, p=2, dim=1)  # ì¿¼ë¦¬ë§ˆë‹¤ í•˜ì§€ ë§ê³  ê°€ëŠ¥í•˜ë©´ ì™¸ë¶€ì—ì„œ 1íšŒë§Œ\n",
        "\n",
        "    # 2) ì¿¼ë¦¬ ì„ë² ë”© (ê·¸ëŒ€ë¡œ OK)\n",
        "    with torch.no_grad():\n",
        "        q = model_embedding.encode(text, convert_to_tensor=True, normalize_embeddings=True)\n",
        "        if q.device != device:\n",
        "            q = q.to(device)\n",
        "        scores = embs_t @ q  # (N,)\n",
        "\n",
        "    N = scores.numel()\n",
        "    if pool_k is None:\n",
        "        pool_k = min(max(top_n * 10, 50), N)\n",
        "\n",
        "    vals, idxs = torch.topk(scores, k=min(pool_k, N))\n",
        "    # vals = vals.clone()  # êµ³ì´ í•„ìš” ì—†ìŒ\n",
        "\n",
        "    # 3) ì†ŒìŠ¤ ê°€ì¤‘ì¹˜ëŠ” ì—¬ê¸°ì„œ 'pool_list' ë§Œë“¤ ë•Œë§Œ 1íšŒ ë°˜ì˜ (ì¤‘ë³µ ê°€ì‚° ì œê±°)\n",
        "    pool_list = []\n",
        "    use_bias = bool(source_bias) and (source_col in df.columns)\n",
        "    for s, i in zip(vals.tolist(), idxs.tolist()):\n",
        "        score = float(s)\n",
        "        if use_bias:\n",
        "            score += float(source_bias.get(str(df.iloc[int(i)][source_col]), 0.0))\n",
        "        pool_list.append((score, int(i)))\n",
        "\n",
        "    # 4) MMRë¡œ ì„ íƒ(ì¤‘ë³µ ì–µì œ)\n",
        "    selected_idxs = []\n",
        "    selected_scores = []\n",
        "\n",
        "    while len(selected_idxs) < min(top_n, len(pool_list)):\n",
        "        best_score, best_i = None, None\n",
        "        for s, i in pool_list:\n",
        "            if i in selected_idxs:\n",
        "                continue\n",
        "            if selected_idxs:\n",
        "                # ì •ê·œí™” ë˜ì–´ ìˆìœ¼ë‹ˆ ë‚´ì ìœ¼ë¡œ ë¹ ë¥´ê²Œ\n",
        "                div = max(float((embs_t[i] @ embs_t[j]).item()) for j in selected_idxs)\n",
        "            else:\n",
        "                div = 0.0\n",
        "            mmr = lambda_mmr * s - (1 - lambda_mmr) * div\n",
        "            if (best_score is None) or (mmr > best_score):\n",
        "                best_score, best_i = mmr, i\n",
        "\n",
        "        if best_i is None:\n",
        "            break\n",
        "\n",
        "        base_sim = float((embs_t[best_i] @ q).item())\n",
        "        if base_sim < min_score and len(selected_idxs) >= 1:\n",
        "            # í›„ë³´ í•˜ë‚˜ ì œê±°í•˜ê³  ê³„ì†\n",
        "            pool_list = [(s, i) for (s, i) in pool_list if i != best_i]\n",
        "            continue\n",
        "\n",
        "        # ì„ íƒ ì§‘í•©ê³¼ ë„ˆë¬´ ìœ ì‚¬í•˜ë©´ ìŠ¤í‚µ\n",
        "        if dedup_sim_thresh and selected_idxs:\n",
        "            max_sim = max(float((embs_t[best_i] @ embs_t[j]).item()) for j in selected_idxs)\n",
        "            if max_sim >= dedup_sim_thresh:\n",
        "                pool_list = [(s, i) for (s, i) in pool_list if i != best_i]\n",
        "                continue\n",
        "\n",
        "        selected_idxs.append(best_i)\n",
        "        selected_scores.append(base_sim)\n",
        "        pool_list = [(s, i) for (s, i) in pool_list if i != best_i]\n",
        "\n",
        "    # 5) ë¶€ì¡±í•˜ë©´ ì„ê³„ì¹˜ ë¬´ì‹œí•˜ê³  ë³´ì¶© (ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ)\n",
        "    if len(selected_idxs) < top_n and pool_list:\n",
        "        for _, i in sorted(pool_list, key=lambda x: x[0], reverse=True):\n",
        "            if i in selected_idxs:\n",
        "                continue\n",
        "            selected_idxs.append(i)\n",
        "            selected_scores.append(float((embs_t[i] @ q).item()))\n",
        "            if len(selected_idxs) >= top_n:\n",
        "                break\n",
        "\n",
        "    # 6) ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±(ë¬¸ì¥ ê²½ê³„ë¡œ íŠ¸ë¦¼)\n",
        "    contexts = []\n",
        "    for i in selected_idxs:\n",
        "        chunk = str(df.iloc[i]['chunk'])\n",
        "        per_limit = max_chars\n",
        "        if source_col in df.columns and max_chars:\n",
        "            st = str(df.iloc[i][source_col])\n",
        "            if st == 'law':\n",
        "                per_limit = int(max_chars * 1.6)\n",
        "            elif st == 'dictionary':\n",
        "                per_limit = max_chars\n",
        "        trimmed = _trim_to_sentence(chunk, per_limit)\n",
        "        if trimmed:\n",
        "            contexts.append(trimmed)\n",
        "\n",
        "    # âœ… ì–´ë–¤ ìƒí™©ì—ì„œë„ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ ë³´ì¥\n",
        "    return contexts if isinstance(contexts, list) else []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPA2e-ZKxRO3"
      },
      "outputs": [],
      "source": [
        "def make_prompt_auto(text, df, embeddings, model_embedding, top_n=3):\n",
        "    # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰\n",
        "    context_chunks = get_context_for_question(text, df, embeddings, model_embedding, top_n=top_n) or []\n",
        "    context_text = \"\\n\\n\".join([f\"{idx+1}. {chunk}\" for idx, chunk in enumerate(context_chunks)]) if context_chunks else \"(ì—†ìŒ)\"\n",
        "\n",
        "    if is_multiple_choice(text):\n",
        "        # ê°ê´€ì‹ íŒŒì‹±\n",
        "        question, options = extract_question_and_choices(text)\n",
        "\n",
        "        # ì˜µì…˜ ì „ì²˜ë¦¬\n",
        "        options = [opt for opt in (opt.strip() for opt in options) if opt]\n",
        "        option_text = \"\\n\".join([f\"{idx}. {opt}\" for idx, opt in enumerate(options, start=1)])\n",
        "\n",
        "        prompt = (\n",
        "            \"[ì°¸ê³  ì •ë³´]\\n\"\n",
        "            f\"{context_text}\\n\\n\"\n",
        "            \"ë‹¹ì‹ ì€ ê¸ˆìœµë³´ì•ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\\n\"\n",
        "            \"ì°¸ê³  ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¹ì‹ ì˜ ì§€ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\\n\"\n",
        "            \"ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ê·¼ê±°ë¥¼ ì œì‹œí•˜ê³ , **ì •ë‹µ ì„ íƒì§€ ë²ˆí˜¸ë§Œ ì¶œë ¥**í•˜ì„¸ìš”.\\n\\n\"\n",
        "            f\"ì§ˆë¬¸: {question}\\n\"\n",
        "            f\"ì„ íƒì§€:\\n{option_text}\\n\\n\"\n",
        "            \"ì •ë‹µ ë²ˆí˜¸:\"\n",
        "        )\n",
        "    else:\n",
        "        prompt = (\n",
        "            \"[ì°¸ê³  ì •ë³´]\\n\"\n",
        "            f\"{context_text}\\n\\n\"\n",
        "            f\"ì§ˆë¬¸: {text}\\n\\n\"\n",
        "            \"ë‹¹ì‹ ì€ ê¸ˆìœµë³´ì•ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\\n\"\n",
        "            \"ì°¸ê³  ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¹ì‹ ì˜ ì§€ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\\n\"\n",
        "            \"ì£¼ê´€ì‹ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\\n\"\n",
        "            \"ë‹µë³€ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”. \\n\"\n",
        "            \"3ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•œ í›„, [ë‹µë³€ì¢…ë£Œ].\\n\\n\"\n",
        "            \"3ë¬¸ì¥ ë‹µë³€: \"\n",
        "        )\n",
        "\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "id": "9PJF6YNqxSNq",
        "outputId": "7e3ea70e-a17e-45e5-e18d-fc7e4f50618d"
      },
      "outputs": [],
      "source": [
        "!pip install -qU transformers accelerate bitsandbytes\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "model_name = \"kakaocorp/kanana-1.5-8b-instruct-2505\"  # ë˜ëŠ” \"kakaocorp/kanana-1.5-8b-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,             # VRAM ì•„ë¼ê¸°\n",
        "    torch_dtype=torch.float16,     # 4bitë©´ fp16 ê¶Œì¥\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "out = pipe(\"ì•ˆë…•! ë„ˆëŠ” ëˆ„êµ¬ì•¼?\", max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.9)\n",
        "print(out[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_VEJman2g8U"
      },
      "outputs": [],
      "source": [
        "# ====== ë™ê·¸ë¼ë¯¸ ìˆ«ì & ë¬¸ì ë§¤í•‘ ======\n",
        "CIRCLED_MAP = {\n",
        "    \"â‘ \": \"1\", \"â‘¡\": \"2\", \"â‘¢\": \"3\", \"â‘£\": \"4\", \"â‘¤\": \"5\",\n",
        "    \"â‘¥\": \"6\", \"â‘¦\": \"7\", \"â‘§\": \"8\", \"â‘¨\": \"9\", \"â‘©\": \"10\",\n",
        "    \"â‘ª\": \"11\", \"â‘«\": \"12\", \"â‘¬\": \"13\", \"â‘­\": \"14\", \"â‘®\": \"15\",\n",
        "    \"â‘¯\": \"16\", \"â‘°\": \"17\", \"â‘±\": \"18\", \"â‘²\": \"19\", \"â‘³\": \"20\",\n",
        "}\n",
        "LETTER_MAP = {ch: str(i) for i, ch in enumerate(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", start=1)}\n",
        "\n",
        "# ====== ì£¼ê´€ì‹ ì •ì œê¸° ======\n",
        "def _normalize_for_compare(s: str) -> str:\n",
        "    s = re.sub(r'\\s+', '', (s or \"\").lower())\n",
        "    s = re.sub(r'[^\\wê°€-í£]', '', s)\n",
        "    return s\n",
        "\n",
        "def _is_echo(line: str, original_question: str) -> bool:\n",
        "    # ğŸ”§ None ê°€ë“œ\n",
        "    if not original_question:\n",
        "        return False\n",
        "    a = _normalize_for_compare(line)\n",
        "    b = _normalize_for_compare(original_question)\n",
        "    if not a or not b:\n",
        "        return False\n",
        "    sa, sb = set(a), set(b)\n",
        "    inter = len(sa & sb); union = len(sa | sb) or 1\n",
        "    return (inter/union) >= 0.8  # ì„ê³„ì¹˜ëŠ” ìƒí™© ë³´ë©° 0.7~0.85 ì¡°ì ˆ\n",
        "\n",
        "def _dedup_sentences(s: str) -> str:\n",
        "    parts = re.split(r'(?<=[.!?ã€‚])\\s+|\\n+', (s or \"\").strip())\n",
        "    seen, out = set(), []\n",
        "    for p in parts:\n",
        "        k = _normalize_for_compare(p)\n",
        "        if k and k not in seen:\n",
        "            seen.add(k)\n",
        "            out.append(p.strip())\n",
        "    return \" \".join(out).strip()\n",
        "\n",
        "# --- ì¼ë°˜ í† í¬ë‚˜ì´ì €(ì–¸ì–´/ë„ë©”ì¸ ë¹„íŠ¹í™”) ---\n",
        "def _tok(s: str):\n",
        "    return set(map(str.lower, re.findall(r'[A-Za-zê°€-í£0-9]{2,}', s or '')))\n",
        "\n",
        "def _has_new_tokens(line: str, question: str, min_new: int = 1) -> bool:\n",
        "    if not question:\n",
        "        return True\n",
        "    return len(_tok(line) - _tok(question)) >= min_new\n",
        "\n",
        "_INSTR_END_RE = re.compile(r'(ì„¸ìš”|ì‹œì˜¤|í•˜ë¼)[\\.\\s]*$')\n",
        "def _is_instructional_line(line: str, original_question: str) -> bool:\n",
        "    if not _has_new_tokens(line, original_question, min_new=1):\n",
        "        return True\n",
        "    if _INSTR_END_RE.search((line or \"\").strip()):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _extract_subjective_answer(text: str, original_question: str = None,\n",
        "                               joiner: str = \", \", max_sentences: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    ì£¼ê´€ì‹: ì²« ìœ íš¨ ì¤„ + ê·¸ ì•„ë˜ ëª©ë¡í˜•(â‘ , 1), (1), A., A) ë“±) ë¼ì¸ë“¤ì„ ëª¨ë‘ í•©ì³ ë°˜í™˜.\n",
        "    ëª©ë¡ì´ ì—†ìœ¼ë©´, ì—ì½”/ëª…ë ¹í˜• ì œì™¸ í›„ ì•ì—ì„œë¶€í„° ìµœëŒ€ Në¬¸ì¥ ë³´ì¶©.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"ë¯¸ì‘ë‹µ\"\n",
        "\n",
        "    # âœ… [ë‹µë³€ì¢…ë£Œ] ì´ì „ê¹Œì§€ë§Œ ì‚¬ìš© (ëŒ€ì†Œë¬¸ì/ê³µë°± í—ˆìš©)\n",
        "    text = re.split(r'\\[ *ë‹µë³€\\s*ì¢…ë£Œ[^\\]]*\\]', text, maxsplit=1)[0]\n",
        "\n",
        "    t = text.strip()\n",
        "    t = re.sub(r'^\\s*(ì •ë‹µ|ë‹µì•ˆ|ë‹µ|Answer|ë‹µë³€)\\s*[:ï¼š\\-]\\s*', '', t, flags=re.I)\n",
        "    t = t.strip('\"\\'' \"â€œâ€â€˜â€™`\")\n",
        "    t = re.sub(r'^\\s*([\\-â€“â€”â€¢Â·]\\s*)+', '', t)\n",
        "\n",
        "    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]\n",
        "    if not lines:\n",
        "        return \"ë¯¸ì‘ë‹µ\"\n",
        "\n",
        "    # 1) ì²« ìœ íš¨ ì¤„\n",
        "    first_idx = None\n",
        "    for i, ln in enumerate(lines):\n",
        "        if _is_echo(ln, original_question):\n",
        "            continue\n",
        "        if _is_instructional_line(ln, original_question):\n",
        "            continue\n",
        "        first_idx = i\n",
        "        break\n",
        "    if first_idx is None:\n",
        "        return \"ë¯¸ì‘ë‹µ\"\n",
        "\n",
        "    collected = [lines[first_idx]]\n",
        "\n",
        "    # 2) ë’¤ì— ì—°ì†ëœ ëª©ë¡í˜• ë¼ì¸ ìˆ˜ì§‘\n",
        "    j = first_idx + 1\n",
        "    took_list = False\n",
        "    while j < len(lines) and OPTION_LINE_RE.match(lines[j]):\n",
        "        collected.append(strip_option_prefix(lines[j]))\n",
        "        took_list = True\n",
        "        j += 1\n",
        "\n",
        "    # 3) ëª©ë¡ì´ ì—†ìœ¼ë©´ ë’¤ìª½ ë¬¸ì¥ ë³´ì¶©\n",
        "    if not took_list:\n",
        "        tail = \" \".join(lines[first_idx+1:])\n",
        "        sents = [s.strip() for s in re.split(r'(?<=[.!?ã€‚])\\s+|\\n+', tail) if s.strip()]\n",
        "        picked = []\n",
        "        for s in sents:\n",
        "            if _is_echo(s, original_question):\n",
        "                continue\n",
        "            if _is_instructional_line(s, original_question):\n",
        "                continue\n",
        "            if any(_normalize_for_compare(s) == _normalize_for_compare(p) for p in picked):\n",
        "                continue\n",
        "            picked.append(s)\n",
        "            if len(picked) >= max_sentences:\n",
        "                break\n",
        "        collected.extend(picked)\n",
        "\n",
        "    out = joiner.join(collected)\n",
        "    out = _dedup_sentences(out)\n",
        "    out = re.sub(r'(.)\\1{3,}', r'\\1\\1', out)\n",
        "    return out[:500] if out else \"ë¯¸ì‘ë‹µ\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_answer_only(generated_text: str, original_question: str) -> str:\n",
        "    \"\"\"\n",
        "    LLM ì¶œë ¥ì—ì„œ 'ì •ë‹µë§Œ' ì¶”ì¶œ\n",
        "      - ì£¼ê´€ì‹: _extract_subjective_answer ì‚¬ìš©\n",
        "      - ê°ê´€ì‹: ìˆ«ì/ë¬¸ì/ì›í˜•ìˆ«ì/`në²ˆ` íŒ¨í„´ ë“± íƒìƒ‰\n",
        "    ë°˜í™˜:\n",
        "      - ê°ê´€ì‹: \"1\"~\"20\" (ê²€ì¶œ ì‹¤íŒ¨ ì‹œ \"0\")\n",
        "      - ì£¼ê´€ì‹: ììœ  í…ìŠ¤íŠ¸(ìµœëŒ€ 300ì), ì—†ìœ¼ë©´ \"ë¯¸ì‘ë‹µ\"\n",
        "    \"\"\"\n",
        "    if not isinstance(generated_text, str) or not generated_text.strip():\n",
        "        return \"ë¯¸ì‘ë‹µ\"\n",
        "\n",
        "    text = generated_text\n",
        "\n",
        "    # \"ë‹µë³€:\" / \"Answer:\" ì´í›„ë§Œ ì‚¬ìš© (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)\n",
        "    m = re.search(r'(ë‹µë³€|Answer)\\s*[:ï¼š\\-]\\s*', text, flags=re.I)\n",
        "    if m:\n",
        "        text = text[m.end():]\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return \"ë¯¸ì‘ë‹µ\"\n",
        "\n",
        "    # ì£¼/ê°ê´€ì‹ ë¶„ê¸°\n",
        "    if not is_multiple_choice(original_question):\n",
        "        return _extract_subjective_answer(text, original_question=original_question)\n",
        "\n",
        "    # ===== ê°ê´€ì‹ ì²˜ë¦¬ =====\n",
        "    num_opts = count_options(original_question)  # ì„ íƒì§€ ê°œìˆ˜(ê²€ì¦ìš©, 0ì´ë©´ ê²€ì¦ ìƒëµ)\n",
        "\n",
        "    # 0) ë™ê·¸ë¼ë¯¸ ìˆ«ì ì •ê·œí™”\n",
        "    for k, v in CIRCLED_MAP.items():\n",
        "        if k in text:\n",
        "            text = text.replace(k, v)\n",
        "\n",
        "    # 1) \"ì •ë‹µ ë²ˆí˜¸: x\" / \"ì •ë‹µ: x\"\n",
        "    m = re.search(r\"ì •ë‹µ\\s*(?:ë²ˆí˜¸)?\\s*[:ï¼š]?\\s*([1-9][0-9]?)(?:\\s*ë²ˆ)?\\b\", text)\n",
        "    if m:\n",
        "        ans = m.group(1)\n",
        "        return ans if (not num_opts or int(ans) <= num_opts) else \"0\"\n",
        "\n",
        "    # 1-2) \"ì •ë‹µ: A\" (ë¬¸ì â†’ ìˆ«ì)\n",
        "    m = re.search(r\"ì •ë‹µ\\s*[:ï¼š]?\\s*([A-Za-z])\\b\", text)\n",
        "    if m:\n",
        "        cand = LETTER_MAP.get(m.group(1).upper(), \"0\")\n",
        "        return cand if (not num_opts or int(cand) <= num_opts) else \"0\"\n",
        "\n",
        "    # 2) ìˆ«ìë§Œ ìˆëŠ” ë‹¨ì¼ ë¼ì¸\n",
        "    for ln in text.splitlines():\n",
        "        ln = ln.strip()\n",
        "        if re.fullmatch(r\"[1-9][0-9]?\", ln):\n",
        "            return ln if (not num_opts or int(ln) <= num_opts) else \"0\"\n",
        "\n",
        "    # 3) ì„ ì§€(ë³´ê¸°) ë¼ì¸ì€ ì œê±°í•˜ê³  ë‹¤ì‹œ íƒìƒ‰\n",
        "    cleaned_lines = []\n",
        "    for ln in text.splitlines():\n",
        "        s = ln.strip()\n",
        "        if OPTION_LINE_RE.match(s):\n",
        "            continue\n",
        "        cleaned_lines.append(s)\n",
        "    s = \" \".join(cleaned_lines)\n",
        "\n",
        "    # 4) ë‹¨ë… ë¬¸ì(A/B/C/...) ìˆìœ¼ë©´ ìˆ«ìë¡œ ë³€í™˜\n",
        "    m = re.search(r\"\\b([A-Za-z])\\b\", s)\n",
        "    if m:\n",
        "        cand = LETTER_MAP.get(m.group(1).upper(), \"0\")\n",
        "        return cand if (not num_opts or int(cand) <= num_opts) else \"0\"\n",
        "\n",
        "    # 5) \"3ë²ˆ\" / ì¼ë°˜ ìˆ«ì\n",
        "    m = re.search(r\"\\b([1-9][0-9]?)\\s*ë²ˆ\\b\", s)\n",
        "    if m:\n",
        "        ans = m.group(1)\n",
        "        return ans if (not num_opts or int(ans) <= num_opts) else \"0\"\n",
        "\n",
        "    m = re.search(r\"\\b([1-9][0-9]?)\\b\", s)\n",
        "    if m:\n",
        "        ans = m.group(1)\n",
        "        return ans if (not num_opts or int(ans) <= num_opts) else \"0\"\n",
        "\n",
        "    # 6) ì‹¤íŒ¨\n",
        "    return \"0\"\n"
      ],
      "metadata": {
        "id": "0fkW01Jf0cgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# q = \"\"\"ê´€ë¦¬ì²´ê³„ ìˆ˜ë¦½ ë° ìš´ì˜'ì˜ 'ì •ì±… ìˆ˜ë¦½' ë‹¨ê³„ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€?\n",
        "# 1 ì •ë³´ë³´í˜¸ ë° ê°œì¸ì •ë³´ë³´í˜¸ ì •ì±…ì˜ ì œÂ·ê°œì •\n",
        "# 2 ê²½ì˜ì§„ì˜ ì°¸ì—¬\n",
        "# 3 ìµœê³ ì±…ì„ìì˜ ì§€ì •\n",
        "# 4 ìì› í• ë‹¹\n",
        "# 5 ë‚´ë¶€ ê°ì‚¬ ì ˆì°¨ì˜ ìˆ˜ë¦½\n",
        "# \"\"\"\n",
        "# print(is_multiple_choice(q))  # True\n",
        "# print(extract_question_and_choices(q))\n",
        "# generated = \"ì •ë‹µ: 2ë²ˆì…ë‹ˆë‹¤. ê²½ì˜ì§„ì˜ ì°¸ì—¬ê°€ í•µì‹¬...\"\n",
        "# print(extract_answer_only(generated, q))  # \"2\"\n",
        "\n",
        "# q2 = \"ì „ìê¸ˆìœµê±°ë˜ë²• ì œ47ì¡°ì˜ ì·¨ì§€ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì‹œì˜¤.\"\n",
        "# generated2 = \"ë‹µë³€: ì „ìê¸ˆìœµê±°ë˜ í˜„í™© íŒŒì•…ê³¼ í†µí™”ì‹ ìš©ì •ì±… ìˆ˜ë¦½ì„ ìœ„í•œ í†µê³„ì¡°ì‚¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.\"\n",
        "# print(is_multiple_choice(q2))  # False\n",
        "# print(extract_answer_only(generated2, q2))\n"
      ],
      "metadata": {
        "id": "Su1jg2ODzx5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1) ë°ì´í„° ë¡œë“œ\n",
        "law_df  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/á„‡á…¥á†¸á„…á…§á†¼á„€á…©á„‰á…µ/ê¸ˆìœµë³´ì•ˆ_ë²•ë¥ _ë°ì´í„°_ì „ì²˜ë¦¬_ìµœì¢….csv')\n",
        "dict_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/csrc_glossary_ko_cleaned_1.csv')\n",
        "econ_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/ì‹œì‚¬ê²½ì œìš©ì–´ì‚¬ì „.csv')\n",
        "ICT_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/merged_ICT_words.csv')\n",
        "\n",
        "# 2) ë²•ë ¹ chunk ë§Œë“¤ê¸°\n",
        "law_df['chunk'] = (\n",
        "    law_df['ë²•ë ¹ëª…'].astype(str) + ' ' +\n",
        "    law_df['ì¡°ë¬¸í—¤ë”'].astype(str) + ' ' +\n",
        "    law_df['í•­ë‚´ìš©'].astype(str)\n",
        ")\n",
        "law_df['source_type'] = 'law'\n",
        "\n",
        "# 3 -a) ìš©ì–´ì‚¬ì „ chunk ë§Œë“¤ê¸°\n",
        "dict_df = dict_df.fillna('')\n",
        "dict_df['chunk'] = (\n",
        "    dict_df['term'].astype(str) + ' ' +\n",
        "    dict_df['aliases'].astype(str) + ' ' +\n",
        "    dict_df['definition_ko'].astype(str)\n",
        ")\n",
        "dict_df['source_type'] = 'dictionary'\n",
        "\n",
        "# 3-b) ì‹œì‚¬ê²½ì œìš©ì–´ì‚¬ì „ chunk ë§Œë“¤ê¸° (ìš©ì–´ + ì„¤ëª…)\n",
        "econ_df = econ_df.fillna('')\n",
        "econ_df['chunk'] = (\n",
        "    econ_df['ìš©ì–´'].astype(str) + ' ' +\n",
        "    econ_df['ì„¤ëª…'].astype(str)\n",
        ")\n",
        "econ_df['source_type'] = 'econ_dictionary'\n",
        "\n",
        "# 3-c) ICT_df chunk ë§Œë“¤ê¸°\n",
        "ICT_df = ICT_df.fillna('')\n",
        "\n",
        "ko = ICT_df['êµ­ë¬¸í‘œì œì–´'].astype(str).str.strip()\n",
        "en = ICT_df['ì˜ë¬¸í‘œì œì–´'].astype(str).str.strip()\n",
        "ab = ICT_df['ì•½ì–´'].astype(str).str.strip()\n",
        "body = ICT_df['ì „ì²˜ë¦¬'].astype(str).str.strip()\n",
        "\n",
        "# ì˜ë¬¸ í‘œì œì–´ëŠ” ê´„í˜¸ ì•ˆì— ë„£ê¸°\n",
        "title = np.where(en != '', ko + ' (' + en + ')', ko)\n",
        "\n",
        "# ì•½ì–´ê°€ ìˆìœ¼ë©´ , (ì•½ì–´) ë¶™ì´ê¸°\n",
        "abbr = np.where(ab != '', ', (' + ab + ')', '')\n",
        "\n",
        "ICT_df['chunk'] = (title + abbr + ' ' + body)\\\n",
        "    .str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "ICT_df['source_type'] = 'ict_dictionary'\n",
        "\n",
        "# 4) í†µí•©\n",
        "merged_df = pd.concat(\n",
        "    [law_df[['chunk','source_type']],\n",
        "    #  dict_df[['chunk','source_type']],\n",
        "     econ_df[['chunk','source_type']],\n",
        "     ICT_df[['chunk','source_type']]],\n",
        "    ignore_index=True\n",
        ").drop_duplicates(subset=['chunk']).reset_index(drop=True)\n",
        "\n",
        "# 5) ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_embedding = SentenceTransformer('jhgan/ko-sroberta-multitask', device=device)\n",
        "\n",
        "# 6) ì „ì²´ chunk ì„ë² ë”© ìƒì„±\n",
        "embeddings = model_embedding.encode(\n",
        "    merged_df['chunk'].tolist(),\n",
        "    batch_size=64,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° í¸í•˜ê²Œ ì •ê·œí™”\n",
        ")\n",
        "\n",
        "# 7) ê¸¸ì´ ê²€ì¦\n",
        "print(\"merged_df ê¸¸ì´:\", len(merged_df))\n",
        "print(\"embeddings ê¸¸ì´:\", embeddings.shape[0])\n",
        "\n",
        "# (ì„ íƒ) ì €ì¥\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/merged_embeddings.npy', embeddings)\n",
        "merged_df.to_csv('/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/merged_chunks.csv', index=False)\n"
      ],
      "metadata": {
        "id": "pNuDgXrcGbOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08b81a11-7825-4bb5-f4a9-b9120f868bff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "corpus_embeddings = torch.from_numpy(\n",
        "    np.load('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/merged_embeddings.npy', allow_pickle=True)\n",
        ").float().to('cuda')"
      ],
      "metadata": {
        "id": "n9lNavc7kslP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "tjCUJ2ydyn0a",
        "outputId": "7e511ea4-29ff-4963-f41e-dca15fddc135"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm\n",
        "\n",
        "# # âœ… ì£¼ê´€ì‹ë§Œ í•„í„°ë§\n",
        "# subjective_df = test.loc[~test['Question'].apply(is_multiple_choice)].copy()\n",
        "\n",
        "# preds_subjective = []\n",
        "\n",
        "# for q in tqdm(subjective_df['Question'], desc=\"Inference (ì£¼ê´€ì‹ë§Œ)\"):\n",
        "#     print(\"=\" * 80)\n",
        "#     print(f\"[Question]\\n{q}\")\n",
        "\n",
        "#     # 1) í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
        "#     prompt = make_prompt_auto(q, merged_df, corpus_embeddings, model_embedding, top_n=3)\n",
        "#     print(\"-\" * 80)\n",
        "#     print(f\"[Prompt Sent to Model]\\n{prompt}\")\n",
        "\n",
        "#     # 2) ëª¨ë¸ ì¶”ë¡ \n",
        "#     output = pipe(prompt, max_new_tokens=256, temperature=0.2, top_p=0.9)\n",
        "#     raw_output = output[0][\"generated_text\"]\n",
        "#     print(\"-\" * 80)\n",
        "#     print(f\"[Raw Model Output]\\n{raw_output}\")\n",
        "\n",
        "#     # 3) ì •ë‹µ ì¶”ì¶œ\n",
        "#     pred_answer = extract_answer_only(raw_output, original_question=q)\n",
        "#     print(\"-\" * 80)\n",
        "#     print(f\"[Extracted Final Answer] -> {pred_answer}\")\n",
        "\n",
        "#     preds_subjective.append(pred_answer)\n",
        "\n",
        "# # ê²°ê³¼ ì €ì¥\n",
        "# subjective_df['Predicted Answer'] = preds_subjective\n"
      ],
      "metadata": {
        "id": "MY_oU594wc4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm\n",
        "\n",
        "# # âœ… ê°ê´€ì‹ë§Œ í•„í„°ë§\n",
        "# subjective_df = test.loc[test['Question'].apply(is_multiple_choice)].copy()\n",
        "\n",
        "# preds_subjective = []\n",
        "\n",
        "# for q in tqdm(subjective_df['Question'], desc=\"Inference (ê°ê´€ì‹ë§Œ)\"):\n",
        "#     print(\"=\" * 80)\n",
        "#     print(f\"[Question]\\n{q}\")\n",
        "\n",
        "#     # 1) í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
        "#     prompt = make_prompt_auto(q, merged_df, corpus_embeddings, model_embedding, top_n=3)\n",
        "#     print(\"-\" * 80)\n",
        "#     print(f\"[Prompt Sent to Model]\\n{prompt}\")\n",
        "\n",
        "#     # 2) ëª¨ë¸ ì¶”ë¡ \n",
        "#     output = pipe(prompt, max_new_tokens=64, temperature=0.2, top_p=0.9)\n",
        "#     raw_output = output[0][\"generated_text\"]\n",
        "#     print(\"-\" * 80)\n",
        "#     print(f\"[Raw Model Output]\\n{raw_output}\")\n",
        "\n",
        "#     # 3) ì •ë‹µ ì¶”ì¶œ\n",
        "#     pred_answer = extract_answer_only(raw_output, original_question=q)\n",
        "#     print(\"-\" * 80)\n",
        "#     print(f\"[Extracted Final Answer] -> {pred_answer}\")\n",
        "\n",
        "#     preds_subjective.append(pred_answer)\n",
        "\n",
        "# # ê²°ê³¼ ì €ì¥\n",
        "# subjective_df['Predicted Answer'] = preds_subjective\n"
      ],
      "metadata": {
        "id": "2gmd9GrAF-9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preds = []\n",
        "\n",
        "# for q in tqdm(test['Question'], desc=\"Inference\"):\n",
        "#     prompt = make_prompt_auto(q, merged_df, corpus_embeddings, model_embedding, top_n=2)  # âœ… corpus_embeddings ì‚¬ìš©\n",
        "#     print(\"=\" * 80)\n",
        "#     print(f\"Q: {q}\")\n",
        "#     print(\"\\n[Prompt]\\n\" + prompt)\n",
        "#     print(\"-\" * 80)\n",
        "\n",
        "#     output = pipe(prompt, max_new_tokens=128, temperature=0.2, top_p=0.9)\n",
        "#     print(\"\\n[Raw Model Output]\\n\", output[0][\"generated_text\"])\n",
        "\n",
        "#     pred_answer = extract_answer_only(output[0][\"generated_text\"], original_question=q)\n",
        "#     print(\"\\n[Extracted Answer] ->\", pred_answer)\n",
        "\n",
        "#     preds.append(pred_answer)\n"
      ],
      "metadata": {
        "id": "5FjNEQXjDD13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# íŒŒì¸íŠœë‹"
      ],
      "metadata": {
        "id": "CVVsevLqv4hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -U transformers accelerate datasets peft"
      ],
      "metadata": {
        "id": "DZunxJLLz0OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import DatasetDict, Dataset\n",
        "# from transformers import TrainingArguments, Trainer\n",
        "# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "# import bitsandbytes as bnb\n",
        "# import json\n",
        "\n",
        "# TRAIN = \"/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹/mcq_train.jsonl\"\n",
        "# VAL   = \"/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹/mcq_val.jsonl\"\n",
        "# BASE  = model_name\n",
        "\n",
        "# def load_jsonl_rows(path):\n",
        "#     with open(path,\"r\",encoding=\"utf-8\") as f:\n",
        "#         return [json.loads(x) for x in f]\n",
        "\n",
        "# train_rows = load_jsonl_rows(TRAIN)\n",
        "# val_rows   = load_jsonl_rows(VAL)\n",
        "\n",
        "# def build_pair(ex):\n",
        "#     # í”„ë¡¬í”„íŠ¸ ê³ ì • (íŠœë‹ ì‹œì—ë„ â€œì •ë‹µ ë²ˆí˜¸ë§Œâ€ ìƒì„±í•˜ë„ë¡)\n",
        "#     prompt = \"ë‹¤ìŒ ê°ê´€ì‹ ë¬¸ì œì˜ ì •ë‹µ ë²ˆí˜¸ë§Œ ì¶œë ¥í•˜ë¼. ì„¤ëª… ê¸ˆì§€.\\n\\n\" + ex[\"input\"] + \"\\n\\nì •ë‹µ ë²ˆí˜¸:\"\n",
        "#     completion = \" \" + ex[\"output\"]  # ì• ê³µë°± í•œ ì¹¸ ë‘ë©´ í•™ìŠµ ì•ˆì •ì \n",
        "#     return {\"prompt\": prompt, \"completion\": completion}\n",
        "\n",
        "# train_pairs = list(map(build_pair, train_rows))\n",
        "# val_pairs   = list(map(build_pair, val_rows))\n",
        "# ds = DatasetDict({\n",
        "#     \"train\": Dataset.from_list(train_pairs),\n",
        "#     \"validation\": Dataset.from_list(val_pairs)\n",
        "# })\n",
        "\n",
        "# tok = AutoTokenizer.from_pretrained(BASE, use_fast=True, trust_remote_code=True)\n",
        "# tok.padding_side = \"left\"\n",
        "# if tok.pad_token_id is None:\n",
        "#     tok.pad_token = tok.eos_token or \"</s>\"\n",
        "\n",
        "# MAX_LEN = 640\n",
        "# IGNORE_INDEX = -100\n",
        "\n",
        "# def tok_mask(batch):\n",
        "#     p = tok(batch[\"prompt\"], add_special_tokens=False)\n",
        "#     c = tok(batch[\"completion\"], add_special_tokens=False)\n",
        "#     input_ids, attn, labels = [], [], []\n",
        "#     for p_ids, c_ids in zip(p[\"input_ids\"], c[\"input_ids\"]):\n",
        "#         max_c = MAX_LEN - len(p_ids)\n",
        "#         if max_c < 1:\n",
        "#             p_ids = p_ids[-(MAX_LEN-1):]\n",
        "#             max_c = 1\n",
        "#         if len(c_ids) > max_c:\n",
        "#             c_ids = c_ids[:max_c]\n",
        "#         ids = p_ids + c_ids\n",
        "#         am  = [1]*len(ids)\n",
        "#         if len(ids) < MAX_LEN:\n",
        "#             pad = MAX_LEN - len(ids)\n",
        "#             ids += [tok.pad_token_id]*pad\n",
        "#             am  += [0]*pad\n",
        "#         lab = [IGNORE_INDEX]*len(p_ids) + c_ids\n",
        "#         if len(lab) < MAX_LEN:\n",
        "#             lab += [IGNORE_INDEX]*(MAX_LEN-len(lab))\n",
        "#         input_ids.append(ids); attn.append(am); labels.append(lab)\n",
        "#     return {\"input_ids\": input_ids, \"attention_mask\": attn, \"labels\": labels}\n",
        "\n",
        "# tok_ds = ds.map(tok_mask, batched=True, remove_columns=[\"prompt\",\"completion\"])\n",
        "\n",
        "# # QLoRA (A100: 4bit + bfloat16 OK)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     BASE, load_in_4bit=True, device_map=\"auto\",\n",
        "#     torch_dtype=torch.bfloat16, trust_remote_code=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True\n",
        "# )\n",
        "# model = prepare_model_for_kbit_training(model)\n",
        "# lora_cfg = LoraConfig(\n",
        "#     r=16, lora_alpha=16, lora_dropout=0.05,\n",
        "#     target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        "# )\n",
        "# model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "# from transformers import TrainingArguments\n",
        "\n",
        "# args = TrainingArguments(\n",
        "#     output_dir=\"/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/á„‘á…¡á„‹á…µá†«á„á…²á„‚á…µá†¼ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º\",\n",
        "#     per_device_train_batch_size=1,\n",
        "#     gradient_accumulation_steps=4,\n",
        "#     num_train_epochs=1,\n",
        "#     learning_rate=2e-5,\n",
        "#     bf16=True,\n",
        "#     logging_steps=10,\n",
        "#     save_steps=100,\n",
        "#     do_eval=True,\n",
        "#     eval_steps=100,\n",
        "#     save_total_limit=2,\n",
        "#     report_to=\"none\"\n",
        "# )\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#     import numpy as np\n",
        "#     preds, labels = eval_pred\n",
        "#     # labelsì—ì„œ í”„ë¡¬í”„íŠ¸(-100) ì œì™¸í•˜ê³  ì²« í† í°ë§Œ í™•ì¸\n",
        "#     # (ì •ë‹µ í•œ ê¸€ìì´ë¯€ë¡œ ì²« ì‹¤ì œ ë¼ë²¨ í† í°ê³¼ ë¹„êµ)\n",
        "#     label_ids = []\n",
        "#     for row in labels:\n",
        "#         row = [x for x in row if x != IGNORE_INDEX]\n",
        "#         label_ids.append(row[0] if row else -1)\n",
        "#     # predsì—ì„œ ê°€ì¥ í™•ë¥  ë†’ì€ í† í° ë½‘ê¸°\n",
        "#     pred_ids = preds.argmax(-1)\n",
        "#     # ì²« ìƒì„± í† í°ë§Œ ë¹„êµ\n",
        "#     hits = 0; total = 0\n",
        "#     for p, l in zip(pred_ids, label_ids):\n",
        "#         if l == -1:\n",
        "#             continue\n",
        "#         total += 1\n",
        "#         if p == l:\n",
        "#             hits += 1\n",
        "#     return {\"em_token\": hits/total if total else 0.0}\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model, args=args,\n",
        "#     train_dataset=tok_ds[\"train\"],\n",
        "#     eval_dataset=tok_ds[\"validation\"],\n",
        "#     tokenizer=tok,\n",
        "#     compute_metrics=compute_metrics\n",
        "# )\n",
        "\n",
        "# trainer.train()\n",
        "# trainer.save_model(\"/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/á„‘á…¡á„‹á…µá†«á„á…²á„‚á…µá†¼ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º\")\n"
      ],
      "metadata": {
        "id": "zbWmhqegv6_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer, pipeline\n",
        "# from peft import AutoPeftModelForCausalLM\n",
        "\n",
        "# ADAPTER_DIR = \"/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹/checkpoint-2777\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR, use_fast=True, trust_remote_code=True)\n",
        "# tokenizer.padding_side = \"left\"\n",
        "# if tokenizer.pad_token_id is None:\n",
        "#     tokenizer.pad_token = tokenizer.eos_token or \"</s>\"\n",
        "\n",
        "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "#     ADAPTER_DIR, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True\n",
        "# ).eval()\n"
      ],
      "metadata": {
        "id": "G82vGjmSv7cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5PB0PeXv7ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "\n",
        "# ë‘ ê°œì˜ pipe ì„¤ì • (ê°ê´€ì‹ / ì£¼ê´€ì‹)\n",
        "pipe_mc = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\",\n",
        "    # ê°ê´€ì‹: ê¸¸ì´ ì§§ê²Œ, temperature ë‚®ê²Œ\n",
        "    max_new_tokens=64,\n",
        "    temperature=0.1,\n",
        "    top_p=0.9\n",
        ")\n",
        "pipe_short = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\",\n",
        "    # ì£¼ê´€ì‹: ê¸¸ì´ ê¸¸ê²Œ, temperature ì¡°ê¸ˆ ë†’ê²Œ\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.2,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "for q in tqdm(test['Question'], desc=\"Inference\"):\n",
        "    # ì§ˆë¬¸ êµ¬ë¶„\n",
        "    if is_multiple_choice(q):\n",
        "        used_pipe = pipe_mc\n",
        "    else:\n",
        "        used_pipe = pipe_short\n",
        "\n",
        "    prompt = make_prompt_auto(q, merged_df, embeddings, model_embedding, top_n=3)\n",
        "    output = used_pipe(prompt)\n",
        "    pred_answer = extract_answer_only(output[0][\"generated_text\"], original_question=q)\n",
        "    preds.append(pred_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC0NL4eTNVXl",
        "outputId": "3266c285-8c31-45d8-9d95-0537b035c101"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MR1QNKo6xTcI",
        "outputId": "42c1d403-fb50-4715-9212-1222a1296b24"
      },
      "outputs": [],
      "source": [
        "# preds = []\n",
        "\n",
        "# for q in tqdm(test['Question'], desc=\"Inference\"):\n",
        "#     prompt = make_prompt_auto(q, merged_df, embeddings, model_embedding, top_n=3)\n",
        "#     output = pipe(prompt, max_new_tokens=128, temperature=0.2, top_p=0.9)\n",
        "#     pred_answer = extract_answer_only(output[0][\"generated_text\"], original_question=q)\n",
        "#     preds.append(pred_answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "id": "T4F5GgiFjQs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8984282-2d17-4d6f-afc7-66f2a576e24c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYgzgrw42jJE"
      },
      "outputs": [],
      "source": [
        "sample_submission = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/sample_submission.csv')\n",
        "sample_submission['Answer'] = preds\n",
        "sample_submission.to_csv('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/ê²°ê³¼ë¬¼/kanana_RAG_ver1.csv', index=False, encoding='utf-8-sig')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKY9IEzvDHme"
      },
      "source": [
        "# ë°ì´í„° ì²­í¬ LAG"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "# jhgan/ko-sroberta-multitask ì´ ëª¨ë¸ì´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²°ê³¼ê°€ ë” ì˜ë‚˜ì˜´\n",
        "model_embedding = SentenceTransformer('jhgan/ko-sroberta-multitask')"
      ],
      "metadata": {
        "id": "p5S_nOUbD5SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldviv_8i4QyG",
        "outputId": "43c231bc-d02a-400a-d2d9-0e1b18ba79b3"
      },
      "outputs": [],
      "source": [
        "# SentenceTransformer ì‚¬ìš©í•´ì„œ ë²¡í„° ì„ë² ë”© ì§„í–‰\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/ë²•ë ¹_ê³ ì‹œ_ë°ì´í„°_ì „ì²˜ë¦¬_3.csv')  # íŒŒì¼ ê²½ë¡œ ë§ê²Œ ìˆ˜ì •\n",
        "\n",
        "# ë²•ë ¹ëª…, ì¡°ë¬¸í—¤ë”, ë³¸ë¬¸ í•©ì¹˜ê¸°\n",
        "df['chunk'] = df['ë²•ë ¹ëª…'].astype(str) + \" \" + df['ì¡°ë¬¸í—¤ë”'].astype(str) + \" \" + df['ë³¸ë¬¸'].astype(str)\n",
        "\n",
        "# ê²°ê³¼ í™•ì¸\n",
        "print(df[['chunk']].head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ì„ë² ë”© ëª¨ë¸ ì´ë¯¸ ì„¤ì • ì™„ë£Œ\n",
        "chunks = df['chunk'].tolist()\n",
        "embeddings = model_embedding.encode(chunks, show_progress_bar=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "G3rmIFNHouvk",
        "outputId": "75bb27ee-0b5c-4485-a5f4-9b88eb3bf1d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiAQRInA4ZNv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/chunk_embeddings_1.npy', embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df['ë²•ë ¹ëª…'].str.contains('ê°œì¸ì •ë³´ ë³´í˜¸ë²•')) & (df['ì¡°ë¬¸í—¤ë”'].str.contains('ì œ2ì¡°'))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "HhIIfGHesKz9",
        "outputId": "60826ef6-f01e-4a0c-9201-9c8626faa367"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eL-0QjW4pn3",
        "outputId": "d6545487-d755-4011-d7a6-8ee99765b462"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import util\n",
        "import numpy as np\n",
        "\n",
        "# ì €ì¥ëœ ì„ë² ë”© ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "embeddings = np.load('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/chunk_embeddings_1.npy')\n",
        "chunks = df['chunk'].tolist()\n",
        "\n",
        "# ì˜ˆì‹œ ì¿¼ë¦¬\n",
        "query = \"ê°œì¸ì •ë³´ ë³´í˜¸ë²•ì´ë€?\"\n",
        "query_emb = model_embedding.encode(query)\n",
        "\n",
        "# ìœ ì‚¬ë„ ê³„ì‚° (cos_scores shape: (N,))\n",
        "cos_scores = util.cos_sim(query_emb, embeddings)[0].cpu().numpy() if hasattr(util.cos_sim(query_emb, embeddings)[0], \"cpu\") else util.cos_sim(query_emb, embeddings)[0]\n",
        "top_n = 10\n",
        "\n",
        "# ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ Top N ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
        "top_idx = np.argsort(-cos_scores)[:top_n]   # <-- ì—¬ê¸°ì„œ ë°”ë¡œ ì¸ë±ìŠ¤ ìƒì„±!\n",
        "\n",
        "print(\"ìœ ì‚¬ë„ê°€ ë†’ì€ ì²­í¬:\")\n",
        "for rank, idx in enumerate(top_idx, 1):\n",
        "    row = df.iloc[int(idx)]\n",
        "    print(f\"[{rank}] {row['ë²•ë ¹ëª…']} | {row['ì¡°ë¬¸í—¤ë”']} | {row['ë³¸ë¬¸'][:120]}... (score: {cos_scores[idx]:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmbSYKym4sV6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}