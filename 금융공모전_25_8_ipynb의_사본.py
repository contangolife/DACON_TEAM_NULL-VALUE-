# -*- coding: utf-8 -*-
"""ê¸ˆìœµê³µëª¨ì „_25_8.ipynbì˜ ì‚¬ë³¸

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19thUBCKn20zahFiFza9fAXZs23b5fyAu

# Baseline model
"""

!pip install -U transformers

!pip install -U bitsandbytes

!pip install --upgrade bitsandbytes

import re
import os
import pandas as pd
from tqdm import tqdm

import torch

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/test.csv')
test #í”„ë¡œí† íƒ€ì…ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì‹¤í—˜ ì§„í–‰ìš© 50ê°œ ì¶”ì¶œ

import re

# ====== ìˆ¨ì€ ë¬¸ì ì œê±° ======
def norm_line(s: str) -> str:
    if not isinstance(s, str):
        return ""
    return (s.replace("\xa0", " ")
              .replace("\u200b", "")
              .replace("\u200c", "")
              .replace("\u200d", "")
              .replace("\ufeff", ""))

# ====== ê°ê´€ì‹ ì„ ì§€ íŒ¨í„´ ======
OPTION_LINE_RE = re.compile(
    r'^[\s\u200b\u200c\u200d\ufeff]*'              # ì„ í–‰ ê³µë°±/ì œë¡œí­
    r'(?:'
    r'  \(\d{1,2}\)'                               # (1) ~ (12)
    r' | \d{1}\s+'                                 # 1â  ë‚´ìš© (1ìë¦¬+ê³µë°± í—ˆìš©)
    r' | \d{1,2}[.)]\s*'                           # 1. / 1) / 12. / 12)
    r' | [â‘ -â‘³]\s*'                                 # â‘ ~â‘³
    r' | [A-F][.)]\s*'                              # A. / A) (ëŒ€ë¬¸ìë§Œ, ê³µë°± ë¶ˆí—ˆ)
    r')',
    re.X
)

def count_options(question_text: str) -> int:
    """
    ì¤„ ë‹¨ìœ„ë¡œ ì„ ì§€ íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ” ë¼ì¸ ê°œìˆ˜
    """
    cnt = 0
    for ln in str(question_text).splitlines():
        ln = norm_line(ln)
        if OPTION_LINE_RE.match(ln):
            cnt += 1
    return cnt

def is_multiple_choice(question_text: str) -> bool:
    """
    ê°ê´€ì‹ ì—¬ë¶€ íŒë‹¨ (ê¸°ë³¸: ì„ ì§€ íŒ¨í„´ 2ê°œ ì´ìƒ)
    """
    cnt = count_options(question_text)
    return cnt >= 2

def strip_option_prefix(line: str) -> str:
    """
    '1) ë‚´ìš©', '(2) ë‚´ìš©', 'A. ë‚´ìš©', 'â‘  ë‚´ìš©'ì—ì„œ ì ‘ë‘ ê¸°í˜¸ ì œê±°
    """
    m = OPTION_LINE_RE.match(line)
    if not m:
        return line.strip()
    return line[m.end():].strip()

def extract_question_and_choices(full_text: str):
    lines = [norm_line(ln).rstrip() for ln in str(full_text).strip().split("\n")]
    q_lines, options = [], []
    for line in lines:
        if OPTION_LINE_RE.match(line):
            options.append(strip_option_prefix(line))
        else:
            q_lines.append(line.strip())
    question = re.sub(r"\s+", " ", " ".join([ln for ln in q_lines if ln]))
    return question, options

import numpy as np
import torch
import torch.nn.functional as F
from sentence_transformers import util
def _trim_to_sentence(text, max_chars):
    """
    ìµœëŒ€ max_chars ê¸¸ì´ê¹Œì§€ ì˜ë¼ë‚´ë˜ ë¬¸ì¥ ê²½ê³„(ë§ˆì¹¨í‘œ, ê°œí–‰ ë“±) ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê¸°
    """
    if not text:
        return ""
    text = str(text).strip()
    if len(text) <= max_chars:
        return text
    # max_charsê¹Œì§€ë§Œ ë³´ê³  ë¬¸ì¥ ì¢…ë£Œë¶€ ì°¾ê¸°
    cutoff = text[:max_chars]
    match = re.findall(r'.*?[.?!]', cutoff)  # ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œê¹Œì§€ í¬í•¨
    if match:
        return "".join(match).strip()
    else:
        return cutoff.strip()

def prepare_corpus_embeddings(embeddings, device=None, pre_normalize=True):
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    embs_t = torch.as_tensor(embeddings, device=device, dtype=torch.float32)
    return F.normalize(embs_t, p=2, dim=1) if pre_normalize else embs_t

def get_context_for_question(
    text,
    df,
    embeddings,                 # torch.Tensor ë˜ëŠ” np.ndarray (ê°€ëŠ¥í•˜ë©´ ë¯¸ë¦¬ prepare_corpus_embeddingsë¡œ ì •ê·œí™”)
    model_embedding,
    top_n=3,
    min_score=0.30,
    max_chars=700,
    pool_k=None,                # ìƒìœ„ í’€ í¬ê¸° (Noneì´ë©´ ìë™)
    lambda_mmr=0.7,             # MMR ê°€ì¤‘
    dedup_sim_thresh=0.95,      # ì„ íƒëœ ì²­í¬ë¼ë¦¬ ìœ ì‚¬ë„ ìƒí•œ
    source_col='source_type',   # 'law' / 'dictionary' ê°™ì€ ì»¬ëŸ¼ëª… (ì—†ìœ¼ë©´ ë¬´ì‹œ)
    source_bias=None,           # ì˜ˆ: {'dictionary': 0.15, 'law': 0.0}
    embeddings_pre_normalized=True,  # embeddingsê°€ ì´ë¯¸ L2 ì •ê·œí™” ë˜ì—ˆëŠ”ì§€
):
    device = embeddings.device if torch.is_tensor(embeddings) else ("cuda" if torch.cuda.is_available() else "cpu")

    # 1) ì½”í¼ìŠ¤ ì„ë² ë”© í…ì„œ ì¤€ë¹„
    if isinstance(embeddings, np.ndarray):
        embs_t = torch.tensor(embeddings, device=device, dtype=torch.float32)
    else:
        embs_t = embeddings.to(device=device, dtype=torch.float32)

    if not embeddings_pre_normalized:
        embs_t = F.normalize(embs_t, p=2, dim=1)  # ì¿¼ë¦¬ë§ˆë‹¤ í•˜ì§€ ë§ê³  ê°€ëŠ¥í•˜ë©´ ì™¸ë¶€ì—ì„œ 1íšŒë§Œ

    # 2) ì¿¼ë¦¬ ì„ë² ë”© (ê·¸ëŒ€ë¡œ OK)
    with torch.no_grad():
        q = model_embedding.encode(text, convert_to_tensor=True, normalize_embeddings=True)
        if q.device != device:
            q = q.to(device)
        scores = embs_t @ q  # (N,)

    N = scores.numel()
    if pool_k is None:
        pool_k = min(max(top_n * 10, 50), N)

    vals, idxs = torch.topk(scores, k=min(pool_k, N))
    # vals = vals.clone()  # êµ³ì´ í•„ìš” ì—†ìŒ

    # 3) ì†ŒìŠ¤ ê°€ì¤‘ì¹˜ëŠ” ì—¬ê¸°ì„œ 'pool_list' ë§Œë“¤ ë•Œë§Œ 1íšŒ ë°˜ì˜ (ì¤‘ë³µ ê°€ì‚° ì œê±°)
    pool_list = []
    use_bias = bool(source_bias) and (source_col in df.columns)
    for s, i in zip(vals.tolist(), idxs.tolist()):
        score = float(s)
        if use_bias:
            score += float(source_bias.get(str(df.iloc[int(i)][source_col]), 0.0))
        pool_list.append((score, int(i)))

    # 4) MMRë¡œ ì„ íƒ(ì¤‘ë³µ ì–µì œ)
    selected_idxs = []
    selected_scores = []

    while len(selected_idxs) < min(top_n, len(pool_list)):
        best_score, best_i = None, None
        for s, i in pool_list:
            if i in selected_idxs:
                continue
            if selected_idxs:
                # ì •ê·œí™” ë˜ì–´ ìˆìœ¼ë‹ˆ ë‚´ì ìœ¼ë¡œ ë¹ ë¥´ê²Œ
                div = max(float((embs_t[i] @ embs_t[j]).item()) for j in selected_idxs)
            else:
                div = 0.0
            mmr = lambda_mmr * s - (1 - lambda_mmr) * div
            if (best_score is None) or (mmr > best_score):
                best_score, best_i = mmr, i

        if best_i is None:
            break

        base_sim = float((embs_t[best_i] @ q).item())
        if base_sim < min_score and len(selected_idxs) >= 1:
            # í›„ë³´ í•˜ë‚˜ ì œê±°í•˜ê³  ê³„ì†
            pool_list = [(s, i) for (s, i) in pool_list if i != best_i]
            continue

        # ì„ íƒ ì§‘í•©ê³¼ ë„ˆë¬´ ìœ ì‚¬í•˜ë©´ ìŠ¤í‚µ
        if dedup_sim_thresh and selected_idxs:
            max_sim = max(float((embs_t[best_i] @ embs_t[j]).item()) for j in selected_idxs)
            if max_sim >= dedup_sim_thresh:
                pool_list = [(s, i) for (s, i) in pool_list if i != best_i]
                continue

        selected_idxs.append(best_i)
        selected_scores.append(base_sim)
        pool_list = [(s, i) for (s, i) in pool_list if i != best_i]

    # 5) ë¶€ì¡±í•˜ë©´ ì„ê³„ì¹˜ ë¬´ì‹œí•˜ê³  ë³´ì¶© (ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ)
    if len(selected_idxs) < top_n and pool_list:
        for _, i in sorted(pool_list, key=lambda x: x[0], reverse=True):
            if i in selected_idxs:
                continue
            selected_idxs.append(i)
            selected_scores.append(float((embs_t[i] @ q).item()))
            if len(selected_idxs) >= top_n:
                break

    # 6) ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±(ë¬¸ì¥ ê²½ê³„ë¡œ íŠ¸ë¦¼)
    contexts = []
    for i in selected_idxs:
        chunk = str(df.iloc[i]['chunk'])
        per_limit = max_chars
        if source_col in df.columns and max_chars:
            st = str(df.iloc[i][source_col])
            if st == 'law':
                per_limit = int(max_chars * 1.6)
            elif st == 'dictionary':
                per_limit = max_chars
        trimmed = _trim_to_sentence(chunk, per_limit)
        if trimmed:
            contexts.append(trimmed)

    # âœ… ì–´ë–¤ ìƒí™©ì—ì„œë„ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ ë³´ì¥
    return contexts if isinstance(contexts, list) else []

def make_prompt_auto(text, df, embeddings, model_embedding, top_n=3):
    # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    context_chunks = get_context_for_question(text, df, embeddings, model_embedding, top_n=top_n) or []
    context_text = "\n\n".join([f"{idx+1}. {chunk}" for idx, chunk in enumerate(context_chunks)]) if context_chunks else "(ì—†ìŒ)"

    if is_multiple_choice(text):
        # ê°ê´€ì‹ íŒŒì‹±
        question, options = extract_question_and_choices(text)

        # ì˜µì…˜ ì „ì²˜ë¦¬
        options = [opt for opt in (opt.strip() for opt in options) if opt]
        option_text = "\n".join([f"{idx}. {opt}" for idx, opt in enumerate(options, start=1)])

        prompt = (
            "[ì°¸ê³  ì •ë³´]\n"
            f"{context_text}\n\n"
            "ë‹¹ì‹ ì€ ê¸ˆìœµë³´ì•ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
            "ì°¸ê³  ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¹ì‹ ì˜ ì§€ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n"
            "ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ê·¼ê±°ë¥¼ ì œì‹œí•˜ê³ , **ì •ë‹µ ì„ íƒì§€ ë²ˆí˜¸ë§Œ ì¶œë ¥**í•˜ì„¸ìš”.\n\n"
            f"ì§ˆë¬¸: {question}\n"
            f"ì„ íƒì§€:\n{option_text}\n\n"
            "ì •ë‹µ ë²ˆí˜¸:"
        )
    else:
        prompt = (
            "[ì°¸ê³  ì •ë³´]\n"
            f"{context_text}\n\n"
            f"ì§ˆë¬¸: {text}\n\n"
            "ë‹¹ì‹ ì€ ê¸ˆìœµë³´ì•ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
            "ì°¸ê³  ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¹ì‹ ì˜ ì§€ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n"
            "ì£¼ê´€ì‹ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n"
            "ë‹µë³€ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”. \n"
            "3ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•œ í›„, [ë‹µë³€ì¢…ë£Œ].\n\n"
            "3ë¬¸ì¥ ë‹µë³€: "
        )

    return prompt

!pip install -qU transformers accelerate bitsandbytes

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

model_name = "kakaocorp/kanana-1.5-8b-instruct-2505"  # ë˜ëŠ” "kakaocorp/kanana-1.5-8b-base"

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=True)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    load_in_4bit=True,             # VRAM ì•„ë¼ê¸°
    torch_dtype=torch.float16,     # 4bitë©´ fp16 ê¶Œì¥
    trust_remote_code=True
)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto"
)

# í…ŒìŠ¤íŠ¸
out = pipe("ì•ˆë…•! ë„ˆëŠ” ëˆ„êµ¬ì•¼?", max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.9)
print(out[0]["generated_text"])

# ====== ë™ê·¸ë¼ë¯¸ ìˆ«ì & ë¬¸ì ë§¤í•‘ ======
CIRCLED_MAP = {
    "â‘ ": "1", "â‘¡": "2", "â‘¢": "3", "â‘£": "4", "â‘¤": "5",
    "â‘¥": "6", "â‘¦": "7", "â‘§": "8", "â‘¨": "9", "â‘©": "10",
    "â‘ª": "11", "â‘«": "12", "â‘¬": "13", "â‘­": "14", "â‘®": "15",
    "â‘¯": "16", "â‘°": "17", "â‘±": "18", "â‘²": "19", "â‘³": "20",
}
LETTER_MAP = {ch: str(i) for i, ch in enumerate("ABCDEFGHIJKLMNOPQRSTUVWXYZ", start=1)}

# ====== ì£¼ê´€ì‹ ì •ì œê¸° ======
def _normalize_for_compare(s: str) -> str:
    s = re.sub(r'\s+', '', (s or "").lower())
    s = re.sub(r'[^\wê°€-í£]', '', s)
    return s

def _is_echo(line: str, original_question: str) -> bool:
    # ğŸ”§ None ê°€ë“œ
    if not original_question:
        return False
    a = _normalize_for_compare(line)
    b = _normalize_for_compare(original_question)
    if not a or not b:
        return False
    sa, sb = set(a), set(b)
    inter = len(sa & sb); union = len(sa | sb) or 1
    return (inter/union) >= 0.8  # ì„ê³„ì¹˜ëŠ” ìƒí™© ë³´ë©° 0.7~0.85 ì¡°ì ˆ

def _dedup_sentences(s: str) -> str:
    parts = re.split(r'(?<=[.!?ã€‚])\s+|\n+', (s or "").strip())
    seen, out = set(), []
    for p in parts:
        k = _normalize_for_compare(p)
        if k and k not in seen:
            seen.add(k)
            out.append(p.strip())
    return " ".join(out).strip()

# --- ì¼ë°˜ í† í¬ë‚˜ì´ì €(ì–¸ì–´/ë„ë©”ì¸ ë¹„íŠ¹í™”) ---
def _tok(s: str):
    return set(map(str.lower, re.findall(r'[A-Za-zê°€-í£0-9]{2,}', s or '')))

def _has_new_tokens(line: str, question: str, min_new: int = 1) -> bool:
    if not question:
        return True
    return len(_tok(line) - _tok(question)) >= min_new

_INSTR_END_RE = re.compile(r'(ì„¸ìš”|ì‹œì˜¤|í•˜ë¼)[\.\s]*$')
def _is_instructional_line(line: str, original_question: str) -> bool:
    if not _has_new_tokens(line, original_question, min_new=1):
        return True
    if _INSTR_END_RE.search((line or "").strip()):
        return True
    return False

def _extract_subjective_answer(text: str, original_question: str = None,
                               joiner: str = ", ", max_sentences: int = 3) -> str:
    """
    ì£¼ê´€ì‹: ì²« ìœ íš¨ ì¤„ + ê·¸ ì•„ë˜ ëª©ë¡í˜•(â‘ , 1), (1), A., A) ë“±) ë¼ì¸ë“¤ì„ ëª¨ë‘ í•©ì³ ë°˜í™˜.
    ëª©ë¡ì´ ì—†ìœ¼ë©´, ì—ì½”/ëª…ë ¹í˜• ì œì™¸ í›„ ì•ì—ì„œë¶€í„° ìµœëŒ€ Në¬¸ì¥ ë³´ì¶©.
    """
    if not isinstance(text, str) or not text.strip():
        return "ë¯¸ì‘ë‹µ"

    # âœ… [ë‹µë³€ì¢…ë£Œ] ì´ì „ê¹Œì§€ë§Œ ì‚¬ìš© (ëŒ€ì†Œë¬¸ì/ê³µë°± í—ˆìš©)
    text = re.split(r'\[ *ë‹µë³€\s*ì¢…ë£Œ[^\]]*\]', text, maxsplit=1)[0]

    t = text.strip()
    t = re.sub(r'^\s*(ì •ë‹µ|ë‹µì•ˆ|ë‹µ|Answer|ë‹µë³€)\s*[:ï¼š\-]\s*', '', t, flags=re.I)
    t = t.strip('"\'' "â€œâ€â€˜â€™`")
    t = re.sub(r'^\s*([\-â€“â€”â€¢Â·]\s*)+', '', t)

    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]
    if not lines:
        return "ë¯¸ì‘ë‹µ"

    # 1) ì²« ìœ íš¨ ì¤„
    first_idx = None
    for i, ln in enumerate(lines):
        if _is_echo(ln, original_question):
            continue
        if _is_instructional_line(ln, original_question):
            continue
        first_idx = i
        break
    if first_idx is None:
        return "ë¯¸ì‘ë‹µ"

    collected = [lines[first_idx]]

    # 2) ë’¤ì— ì—°ì†ëœ ëª©ë¡í˜• ë¼ì¸ ìˆ˜ì§‘
    j = first_idx + 1
    took_list = False
    while j < len(lines) and OPTION_LINE_RE.match(lines[j]):
        collected.append(strip_option_prefix(lines[j]))
        took_list = True
        j += 1

    # 3) ëª©ë¡ì´ ì—†ìœ¼ë©´ ë’¤ìª½ ë¬¸ì¥ ë³´ì¶©
    if not took_list:
        tail = " ".join(lines[first_idx+1:])
        sents = [s.strip() for s in re.split(r'(?<=[.!?ã€‚])\s+|\n+', tail) if s.strip()]
        picked = []
        for s in sents:
            if _is_echo(s, original_question):
                continue
            if _is_instructional_line(s, original_question):
                continue
            if any(_normalize_for_compare(s) == _normalize_for_compare(p) for p in picked):
                continue
            picked.append(s)
            if len(picked) >= max_sentences:
                break
        collected.extend(picked)

    out = joiner.join(collected)
    out = _dedup_sentences(out)
    out = re.sub(r'(.)\1{3,}', r'\1\1', out)
    return out[:500] if out else "ë¯¸ì‘ë‹µ"

def extract_answer_only(generated_text: str, original_question: str) -> str:
    """
    LLM ì¶œë ¥ì—ì„œ 'ì •ë‹µë§Œ' ì¶”ì¶œ
      - ì£¼ê´€ì‹: _extract_subjective_answer ì‚¬ìš©
      - ê°ê´€ì‹: ìˆ«ì/ë¬¸ì/ì›í˜•ìˆ«ì/`në²ˆ` íŒ¨í„´ ë“± íƒìƒ‰
    ë°˜í™˜:
      - ê°ê´€ì‹: "1"~"20" (ê²€ì¶œ ì‹¤íŒ¨ ì‹œ "0")
      - ì£¼ê´€ì‹: ììœ  í…ìŠ¤íŠ¸(ìµœëŒ€ 300ì), ì—†ìœ¼ë©´ "ë¯¸ì‘ë‹µ"
    """
    if not isinstance(generated_text, str) or not generated_text.strip():
        return "ë¯¸ì‘ë‹µ"

    text = generated_text

    # "ë‹µë³€:" / "Answer:" ì´í›„ë§Œ ì‚¬ìš© (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ìŒ)
    m = re.search(r'(ë‹µë³€|Answer)\s*[:ï¼š\-]\s*', text, flags=re.I)
    if m:
        text = text[m.end():]
    text = text.strip()
    if not text:
        return "ë¯¸ì‘ë‹µ"

    # ì£¼/ê°ê´€ì‹ ë¶„ê¸°
    if not is_multiple_choice(original_question):
        return _extract_subjective_answer(text, original_question=original_question)

    # ===== ê°ê´€ì‹ ì²˜ë¦¬ =====
    num_opts = count_options(original_question)  # ì„ íƒì§€ ê°œìˆ˜(ê²€ì¦ìš©, 0ì´ë©´ ê²€ì¦ ìƒëµ)

    # 0) ë™ê·¸ë¼ë¯¸ ìˆ«ì ì •ê·œí™”
    for k, v in CIRCLED_MAP.items():
        if k in text:
            text = text.replace(k, v)

    # 1) "ì •ë‹µ ë²ˆí˜¸: x" / "ì •ë‹µ: x"
    m = re.search(r"ì •ë‹µ\s*(?:ë²ˆí˜¸)?\s*[:ï¼š]?\s*([1-9][0-9]?)(?:\s*ë²ˆ)?\b", text)
    if m:
        ans = m.group(1)
        return ans if (not num_opts or int(ans) <= num_opts) else "0"

    # 1-2) "ì •ë‹µ: A" (ë¬¸ì â†’ ìˆ«ì)
    m = re.search(r"ì •ë‹µ\s*[:ï¼š]?\s*([A-Za-z])\b", text)
    if m:
        cand = LETTER_MAP.get(m.group(1).upper(), "0")
        return cand if (not num_opts or int(cand) <= num_opts) else "0"

    # 2) ìˆ«ìë§Œ ìˆëŠ” ë‹¨ì¼ ë¼ì¸
    for ln in text.splitlines():
        ln = ln.strip()
        if re.fullmatch(r"[1-9][0-9]?", ln):
            return ln if (not num_opts or int(ln) <= num_opts) else "0"

    # 3) ì„ ì§€(ë³´ê¸°) ë¼ì¸ì€ ì œê±°í•˜ê³  ë‹¤ì‹œ íƒìƒ‰
    cleaned_lines = []
    for ln in text.splitlines():
        s = ln.strip()
        if OPTION_LINE_RE.match(s):
            continue
        cleaned_lines.append(s)
    s = " ".join(cleaned_lines)

    # 4) ë‹¨ë… ë¬¸ì(A/B/C/...) ìˆìœ¼ë©´ ìˆ«ìë¡œ ë³€í™˜
    m = re.search(r"\b([A-Za-z])\b", s)
    if m:
        cand = LETTER_MAP.get(m.group(1).upper(), "0")
        return cand if (not num_opts or int(cand) <= num_opts) else "0"

    # 5) "3ë²ˆ" / ì¼ë°˜ ìˆ«ì
    m = re.search(r"\b([1-9][0-9]?)\s*ë²ˆ\b", s)
    if m:
        ans = m.group(1)
        return ans if (not num_opts or int(ans) <= num_opts) else "0"

    m = re.search(r"\b([1-9][0-9]?)\b", s)
    if m:
        ans = m.group(1)
        return ans if (not num_opts or int(ans) <= num_opts) else "0"

    # 6) ì‹¤íŒ¨
    return "0"

# q = """ê´€ë¦¬ì²´ê³„ ìˆ˜ë¦½ ë° ìš´ì˜'ì˜ 'ì •ì±… ìˆ˜ë¦½' ë‹¨ê³„ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†ŒëŠ” ë¬´ì—‡ì¸ê°€?
# 1 ì •ë³´ë³´í˜¸ ë° ê°œì¸ì •ë³´ë³´í˜¸ ì •ì±…ì˜ ì œÂ·ê°œì •
# 2 ê²½ì˜ì§„ì˜ ì°¸ì—¬
# 3 ìµœê³ ì±…ì„ìì˜ ì§€ì •
# 4 ìì› í• ë‹¹
# 5 ë‚´ë¶€ ê°ì‚¬ ì ˆì°¨ì˜ ìˆ˜ë¦½
# """
# print(is_multiple_choice(q))  # True
# print(extract_question_and_choices(q))
# generated = "ì •ë‹µ: 2ë²ˆì…ë‹ˆë‹¤. ê²½ì˜ì§„ì˜ ì°¸ì—¬ê°€ í•µì‹¬..."
# print(extract_answer_only(generated, q))  # "2"

# q2 = "ì „ìê¸ˆìœµê±°ë˜ë²• ì œ47ì¡°ì˜ ì·¨ì§€ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì‹œì˜¤."
# generated2 = "ë‹µë³€: ì „ìê¸ˆìœµê±°ë˜ í˜„í™© íŒŒì•…ê³¼ í†µí™”ì‹ ìš©ì •ì±… ìˆ˜ë¦½ì„ ìœ„í•œ í†µê³„ì¡°ì‚¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤."
# print(is_multiple_choice(q2))  # False
# print(extract_answer_only(generated2, q2))

import pandas as pd
import numpy as np
import torch
from sentence_transformers import SentenceTransformer

# 1) ë°ì´í„° ë¡œë“œ
law_df  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/á„‡á…¥á†¸á„…á…§á†¼á„€á…©á„‰á…µ/ê¸ˆìœµë³´ì•ˆ_ë²•ë¥ _ë°ì´í„°_ì „ì²˜ë¦¬_ìµœì¢….csv')
dict_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/csrc_glossary_ko_cleaned_1.csv')
econ_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/ì‹œì‚¬ê²½ì œìš©ì–´ì‚¬ì „.csv')
ICT_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/merged_ICT_words.csv')

# 2) ë²•ë ¹ chunk ë§Œë“¤ê¸°
law_df['chunk'] = (
    law_df['ë²•ë ¹ëª…'].astype(str) + ' ' +
    law_df['ì¡°ë¬¸í—¤ë”'].astype(str) + ' ' +
    law_df['í•­ë‚´ìš©'].astype(str)
)
law_df['source_type'] = 'law'

# 3 -a) ìš©ì–´ì‚¬ì „ chunk ë§Œë“¤ê¸°
dict_df = dict_df.fillna('')
dict_df['chunk'] = (
    dict_df['term'].astype(str) + ' ' +
    dict_df['aliases'].astype(str) + ' ' +
    dict_df['definition_ko'].astype(str)
)
dict_df['source_type'] = 'dictionary'

# 3-b) ì‹œì‚¬ê²½ì œìš©ì–´ì‚¬ì „ chunk ë§Œë“¤ê¸° (ìš©ì–´ + ì„¤ëª…)
econ_df = econ_df.fillna('')
econ_df['chunk'] = (
    econ_df['ìš©ì–´'].astype(str) + ' ' +
    econ_df['ì„¤ëª…'].astype(str)
)
econ_df['source_type'] = 'econ_dictionary'

# 3-c) ICT_df chunk ë§Œë“¤ê¸°
ICT_df = ICT_df.fillna('')

ko = ICT_df['êµ­ë¬¸í‘œì œì–´'].astype(str).str.strip()
en = ICT_df['ì˜ë¬¸í‘œì œì–´'].astype(str).str.strip()
ab = ICT_df['ì•½ì–´'].astype(str).str.strip()
body = ICT_df['ì „ì²˜ë¦¬'].astype(str).str.strip()

# ì˜ë¬¸ í‘œì œì–´ëŠ” ê´„í˜¸ ì•ˆì— ë„£ê¸°
title = np.where(en != '', ko + ' (' + en + ')', ko)

# ì•½ì–´ê°€ ìˆìœ¼ë©´ , (ì•½ì–´) ë¶™ì´ê¸°
abbr = np.where(ab != '', ', (' + ab + ')', '')

ICT_df['chunk'] = (title + abbr + ' ' + body)\
    .str.replace(r'\s+', ' ', regex=True).str.strip()
ICT_df['source_type'] = 'ict_dictionary'

# 4) í†µí•©
merged_df = pd.concat(
    [law_df[['chunk','source_type']],
    #  dict_df[['chunk','source_type']],
     econ_df[['chunk','source_type']],
     ICT_df[['chunk','source_type']]],
    ignore_index=True
).drop_duplicates(subset=['chunk']).reset_index(drop=True)

# 5) ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
device = "cuda" if torch.cuda.is_available() else "cpu"
model_embedding = SentenceTransformer('jhgan/ko-sroberta-multitask', device=device)

# 6) ì „ì²´ chunk ì„ë² ë”© ìƒì„±
embeddings = model_embedding.encode(
    merged_df['chunk'].tolist(),
    batch_size=64,
    convert_to_numpy=True,
    normalize_embeddings=True  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° í¸í•˜ê²Œ ì •ê·œí™”
)

# 7) ê¸¸ì´ ê²€ì¦
print("merged_df ê¸¸ì´:", len(merged_df))
print("embeddings ê¸¸ì´:", embeddings.shape[0])

# (ì„ íƒ) ì €ì¥
np.save('/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/merged_embeddings.npy', embeddings)
merged_df.to_csv('/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/merged_chunks.csv', index=False)

import torch
import numpy as np

corpus_embeddings = torch.from_numpy(
    np.load('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/merged_embeddings.npy', allow_pickle=True)
).float().to('cuda')

merged_df

# from tqdm import tqdm

# # âœ… ì£¼ê´€ì‹ë§Œ í•„í„°ë§
# subjective_df = test.loc[~test['Question'].apply(is_multiple_choice)].copy()

# preds_subjective = []

# for q in tqdm(subjective_df['Question'], desc="Inference (ì£¼ê´€ì‹ë§Œ)"):
#     print("=" * 80)
#     print(f"[Question]\n{q}")

#     # 1) í”„ë¡¬í”„íŠ¸ ìƒì„±
#     prompt = make_prompt_auto(q, merged_df, corpus_embeddings, model_embedding, top_n=3)
#     print("-" * 80)
#     print(f"[Prompt Sent to Model]\n{prompt}")

#     # 2) ëª¨ë¸ ì¶”ë¡ 
#     output = pipe(prompt, max_new_tokens=256, temperature=0.2, top_p=0.9)
#     raw_output = output[0]["generated_text"]
#     print("-" * 80)
#     print(f"[Raw Model Output]\n{raw_output}")

#     # 3) ì •ë‹µ ì¶”ì¶œ
#     pred_answer = extract_answer_only(raw_output, original_question=q)
#     print("-" * 80)
#     print(f"[Extracted Final Answer] -> {pred_answer}")

#     preds_subjective.append(pred_answer)

# # ê²°ê³¼ ì €ì¥
# subjective_df['Predicted Answer'] = preds_subjective

# from tqdm import tqdm

# # âœ… ê°ê´€ì‹ë§Œ í•„í„°ë§
# subjective_df = test.loc[test['Question'].apply(is_multiple_choice)].copy()

# preds_subjective = []

# for q in tqdm(subjective_df['Question'], desc="Inference (ê°ê´€ì‹ë§Œ)"):
#     print("=" * 80)
#     print(f"[Question]\n{q}")

#     # 1) í”„ë¡¬í”„íŠ¸ ìƒì„±
#     prompt = make_prompt_auto(q, merged_df, corpus_embeddings, model_embedding, top_n=3)
#     print("-" * 80)
#     print(f"[Prompt Sent to Model]\n{prompt}")

#     # 2) ëª¨ë¸ ì¶”ë¡ 
#     output = pipe(prompt, max_new_tokens=64, temperature=0.2, top_p=0.9)
#     raw_output = output[0]["generated_text"]
#     print("-" * 80)
#     print(f"[Raw Model Output]\n{raw_output}")

#     # 3) ì •ë‹µ ì¶”ì¶œ
#     pred_answer = extract_answer_only(raw_output, original_question=q)
#     print("-" * 80)
#     print(f"[Extracted Final Answer] -> {pred_answer}")

#     preds_subjective.append(pred_answer)

# # ê²°ê³¼ ì €ì¥
# subjective_df['Predicted Answer'] = preds_subjective

# preds = []

# for q in tqdm(test['Question'], desc="Inference"):
#     prompt = make_prompt_auto(q, merged_df, corpus_embeddings, model_embedding, top_n=2)  # âœ… corpus_embeddings ì‚¬ìš©
#     print("=" * 80)
#     print(f"Q: {q}")
#     print("\n[Prompt]\n" + prompt)
#     print("-" * 80)

#     output = pipe(prompt, max_new_tokens=128, temperature=0.2, top_p=0.9)
#     print("\n[Raw Model Output]\n", output[0]["generated_text"])

#     pred_answer = extract_answer_only(output[0]["generated_text"], original_question=q)
#     print("\n[Extracted Answer] ->", pred_answer)

#     preds.append(pred_answer)

"""# íŒŒì¸íŠœë‹"""

# pip install -U transformers accelerate datasets peft

# from datasets import DatasetDict, Dataset
# from transformers import TrainingArguments, Trainer
# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
# import bitsandbytes as bnb
# import json

# TRAIN = "/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹/mcq_train.jsonl"
# VAL   = "/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹/mcq_val.jsonl"
# BASE  = model_name

# def load_jsonl_rows(path):
#     with open(path,"r",encoding="utf-8") as f:
#         return [json.loads(x) for x in f]

# train_rows = load_jsonl_rows(TRAIN)
# val_rows   = load_jsonl_rows(VAL)

# def build_pair(ex):
#     # í”„ë¡¬í”„íŠ¸ ê³ ì • (íŠœë‹ ì‹œì—ë„ â€œì •ë‹µ ë²ˆí˜¸ë§Œâ€ ìƒì„±í•˜ë„ë¡)
#     prompt = "ë‹¤ìŒ ê°ê´€ì‹ ë¬¸ì œì˜ ì •ë‹µ ë²ˆí˜¸ë§Œ ì¶œë ¥í•˜ë¼. ì„¤ëª… ê¸ˆì§€.\n\n" + ex["input"] + "\n\nì •ë‹µ ë²ˆí˜¸:"
#     completion = " " + ex["output"]  # ì• ê³µë°± í•œ ì¹¸ ë‘ë©´ í•™ìŠµ ì•ˆì •ì 
#     return {"prompt": prompt, "completion": completion}

# train_pairs = list(map(build_pair, train_rows))
# val_pairs   = list(map(build_pair, val_rows))
# ds = DatasetDict({
#     "train": Dataset.from_list(train_pairs),
#     "validation": Dataset.from_list(val_pairs)
# })

# tok = AutoTokenizer.from_pretrained(BASE, use_fast=True, trust_remote_code=True)
# tok.padding_side = "left"
# if tok.pad_token_id is None:
#     tok.pad_token = tok.eos_token or "</s>"

# MAX_LEN = 640
# IGNORE_INDEX = -100

# def tok_mask(batch):
#     p = tok(batch["prompt"], add_special_tokens=False)
#     c = tok(batch["completion"], add_special_tokens=False)
#     input_ids, attn, labels = [], [], []
#     for p_ids, c_ids in zip(p["input_ids"], c["input_ids"]):
#         max_c = MAX_LEN - len(p_ids)
#         if max_c < 1:
#             p_ids = p_ids[-(MAX_LEN-1):]
#             max_c = 1
#         if len(c_ids) > max_c:
#             c_ids = c_ids[:max_c]
#         ids = p_ids + c_ids
#         am  = [1]*len(ids)
#         if len(ids) < MAX_LEN:
#             pad = MAX_LEN - len(ids)
#             ids += [tok.pad_token_id]*pad
#             am  += [0]*pad
#         lab = [IGNORE_INDEX]*len(p_ids) + c_ids
#         if len(lab) < MAX_LEN:
#             lab += [IGNORE_INDEX]*(MAX_LEN-len(lab))
#         input_ids.append(ids); attn.append(am); labels.append(lab)
#     return {"input_ids": input_ids, "attention_mask": attn, "labels": labels}

# tok_ds = ds.map(tok_mask, batched=True, remove_columns=["prompt","completion"])

# # QLoRA (A100: 4bit + bfloat16 OK)
# model = AutoModelForCausalLM.from_pretrained(
#     BASE, load_in_4bit=True, device_map="auto",
#     torch_dtype=torch.bfloat16, trust_remote_code=True,
#     bnb_4bit_quant_type="nf4", bnb_4bit_use_double_quant=True
# )
# model = prepare_model_for_kbit_training(model)
# lora_cfg = LoraConfig(
#     r=16, lora_alpha=16, lora_dropout=0.05,
#     target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
# )
# model = get_peft_model(model, lora_cfg)

# from transformers import TrainingArguments

# args = TrainingArguments(
#     output_dir="/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/á„‘á…¡á„‹á…µá†«á„á…²á„‚á…µá†¼ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º",
#     per_device_train_batch_size=1,
#     gradient_accumulation_steps=4,
#     num_train_epochs=1,
#     learning_rate=2e-5,
#     bf16=True,
#     logging_steps=10,
#     save_steps=100,
#     do_eval=True,
#     eval_steps=100,
#     save_total_limit=2,
#     report_to="none"
# )

# def compute_metrics(eval_pred):
#     import numpy as np
#     preds, labels = eval_pred
#     # labelsì—ì„œ í”„ë¡¬í”„íŠ¸(-100) ì œì™¸í•˜ê³  ì²« í† í°ë§Œ í™•ì¸
#     # (ì •ë‹µ í•œ ê¸€ìì´ë¯€ë¡œ ì²« ì‹¤ì œ ë¼ë²¨ í† í°ê³¼ ë¹„êµ)
#     label_ids = []
#     for row in labels:
#         row = [x for x in row if x != IGNORE_INDEX]
#         label_ids.append(row[0] if row else -1)
#     # predsì—ì„œ ê°€ì¥ í™•ë¥  ë†’ì€ í† í° ë½‘ê¸°
#     pred_ids = preds.argmax(-1)
#     # ì²« ìƒì„± í† í°ë§Œ ë¹„êµ
#     hits = 0; total = 0
#     for p, l in zip(pred_ids, label_ids):
#         if l == -1:
#             continue
#         total += 1
#         if p == l:
#             hits += 1
#     return {"em_token": hits/total if total else 0.0}

# trainer = Trainer(
#     model=model, args=args,
#     train_dataset=tok_ds["train"],
#     eval_dataset=tok_ds["validation"],
#     tokenizer=tok,
#     compute_metrics=compute_metrics
# )

# trainer.train()
# trainer.save_model("/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/á„‘á…¡á„‹á…µá†«á„á…²á„‚á…µá†¼ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º")

# from transformers import AutoTokenizer, pipeline
# from peft import AutoPeftModelForCausalLM

# ADAPTER_DIR = "/content/drive/MyDrive/Colab Notebooks/ê¸ˆìœµê³µëª¨ì „_25_8/íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹/checkpoint-2777"

# tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR, use_fast=True, trust_remote_code=True)
# tokenizer.padding_side = "left"
# if tokenizer.pad_token_id is None:
#     tokenizer.pad_token = tokenizer.eos_token or "</s>"

# model = AutoPeftModelForCausalLM.from_pretrained(
#     ADAPTER_DIR, device_map="auto", torch_dtype="auto", trust_remote_code=True
# ).eval()



preds = []

# ë‘ ê°œì˜ pipe ì„¤ì • (ê°ê´€ì‹ / ì£¼ê´€ì‹)
pipe_mc = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto",
    # ê°ê´€ì‹: ê¸¸ì´ ì§§ê²Œ, temperature ë‚®ê²Œ
    max_new_tokens=64,
    temperature=0.1,
    top_p=0.9
)
pipe_short = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto",
    # ì£¼ê´€ì‹: ê¸¸ì´ ê¸¸ê²Œ, temperature ì¡°ê¸ˆ ë†’ê²Œ
    max_new_tokens=256,
    temperature=0.2,
    top_p=0.9,
)

for q in tqdm(test['Question'], desc="Inference"):
    # ì§ˆë¬¸ êµ¬ë¶„
    if is_multiple_choice(q):
        used_pipe = pipe_mc
    else:
        used_pipe = pipe_short

    prompt = make_prompt_auto(q, merged_df, embeddings, model_embedding, top_n=3)
    output = used_pipe(prompt)
    pred_answer = extract_answer_only(output[0]["generated_text"], original_question=q)
    preds.append(pred_answer)

# preds = []

# for q in tqdm(test['Question'], desc="Inference"):
#     prompt = make_prompt_auto(q, merged_df, embeddings, model_embedding, top_n=3)
#     output = pipe(prompt, max_new_tokens=128, temperature=0.2, top_p=0.9)
#     pred_answer = extract_answer_only(output[0]["generated_text"], original_question=q)
#     preds.append(pred_answer)

preds

sample_submission = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/sample_submission.csv')
sample_submission['Answer'] = preds
sample_submission.to_csv('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/ê²°ê³¼ë¬¼/kanana_RAG_ver1.csv', index=False, encoding='utf-8-sig')

"""# ë°ì´í„° ì²­í¬ LAG"""

from sentence_transformers import SentenceTransformer
# jhgan/ko-sroberta-multitask ì´ ëª¨ë¸ì´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê²°ê³¼ê°€ ë” ì˜ë‚˜ì˜´
model_embedding = SentenceTransformer('jhgan/ko-sroberta-multitask')

# SentenceTransformer ì‚¬ìš©í•´ì„œ ë²¡í„° ì„ë² ë”© ì§„í–‰
import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/ë²•ë ¹_ê³ ì‹œ_ë°ì´í„°_ì „ì²˜ë¦¬_3.csv')  # íŒŒì¼ ê²½ë¡œ ë§ê²Œ ìˆ˜ì •

# ë²•ë ¹ëª…, ì¡°ë¬¸í—¤ë”, ë³¸ë¬¸ í•©ì¹˜ê¸°
df['chunk'] = df['ë²•ë ¹ëª…'].astype(str) + " " + df['ì¡°ë¬¸í—¤ë”'].astype(str) + " " + df['ë³¸ë¬¸'].astype(str)

# ê²°ê³¼ í™•ì¸
print(df[['chunk']].head())

#ì„ë² ë”© ëª¨ë¸ ì´ë¯¸ ì„¤ì • ì™„ë£Œ
chunks = df['chunk'].tolist()
embeddings = model_embedding.encode(chunks, show_progress_bar=True)

import numpy as np

np.save('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/chunk_embeddings_1.npy', embeddings)

df[(df['ë²•ë ¹ëª…'].str.contains('ê°œì¸ì •ë³´ ë³´í˜¸ë²•')) & (df['ì¡°ë¬¸í—¤ë”'].str.contains('ì œ2ì¡°'))]

from sentence_transformers import util
import numpy as np

# ì €ì¥ëœ ì„ë² ë”© ë¶ˆëŸ¬ì˜¤ê¸°
embeddings = np.load('/content/drive/MyDrive/Colab Notebooks/á„€á…³á†·á„‹á…²á†¼á„€á…©á†¼á„†á…©á„Œá…¥á†«_25_8/chunk_embeddings_1.npy')
chunks = df['chunk'].tolist()

# ì˜ˆì‹œ ì¿¼ë¦¬
query = "ê°œì¸ì •ë³´ ë³´í˜¸ë²•ì´ë€?"
query_emb = model_embedding.encode(query)

# ìœ ì‚¬ë„ ê³„ì‚° (cos_scores shape: (N,))
cos_scores = util.cos_sim(query_emb, embeddings)[0].cpu().numpy() if hasattr(util.cos_sim(query_emb, embeddings)[0], "cpu") else util.cos_sim(query_emb, embeddings)[0]
top_n = 10

# ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ Top N ì¸ë±ìŠ¤ ì¶”ì¶œ
top_idx = np.argsort(-cos_scores)[:top_n]   # <-- ì—¬ê¸°ì„œ ë°”ë¡œ ì¸ë±ìŠ¤ ìƒì„±!

print("ìœ ì‚¬ë„ê°€ ë†’ì€ ì²­í¬:")
for rank, idx in enumerate(top_idx, 1):
    row = df.iloc[int(idx)]
    print(f"[{rank}] {row['ë²•ë ¹ëª…']} | {row['ì¡°ë¬¸í—¤ë”']} | {row['ë³¸ë¬¸'][:120]}... (score: {cos_scores[idx]:.4f})")

